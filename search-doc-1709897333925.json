{"searchDocs":[{"title":"Apache Kafka meets Apache Wayang - Part 1","type":0,"sectionRef":"#","url":"/blog/kafka-meets-wayang-1","content":"","keywords":"","version":null},{"title":"Intro​","type":1,"pageTitle":"Apache Kafka meets Apache Wayang - Part 1","url":"/blog/kafka-meets-wayang-1#intro","content":" This article is the first of a four part series about federated data analysis using Apache Wayang. The first article starts with an introduction of a typical data colaboration scenario which will emerge in our digital future.  In part two and three we will share a summary of our Apache Kafka client implementation for Apache Wayang. We started with the Java Platform (part 2) and the Apache Spark implementation follows (W.I.P.) in part three.  The use case behind this work is an imaginary data collaboration scenario. We see this example and the demand for a solution already in many places. For us this is motivation enough to propose a solution. This would also allow us to do more local data processing, and businesses can stop moving data around the world, but rather care about data locality while they expose and share specific information to others by using data federation. This reduces complexity of data management and cost dramatically.  For this purpose, we illustrate a cross organizational data sharing scenario from the finance sector soon. This analysis pattern will also be relevant in the context of data analysis along supply chains, another typical example where data from many stakeholder together is needed but never managed in one place, for good reasons.  Data federation can help us to unlock the hidden value of all those isolated data lakes.  ","version":null,"tagName":"h2"},{"title":"A cross organizational data sharing scenario​","type":1,"pageTitle":"Apache Kafka meets Apache Wayang - Part 1","url":"/blog/kafka-meets-wayang-1#a-cross-organizational-data-sharing-scenario","content":" Our goal is the implementation of a cross organization decentralized data processing scenario, in which protected local data should be processed in combination with public data from public sources in a collaborative manner. Instead of copying all data into a central data lake or a central data platform we decided to use federated analytics. Apache Wayang is the tool we work with. In our case, the public data is hosted on publicly available websites or data pods. A client can use the HTTP(S) protocol to read the data which is given in a well defined format. For simplicity we decided to use CSV format. When we look into the data of each participant we have a different perspective.  Our processing procedure should calculate a particular metric on the local data of each participant. An example of such a metric is the average spending of all users on a particular product category per month. This can vary from partner to partner, hence, we want to be able to calculate a peer-group comparison so that each partner can see its own metric compared with a global average calculated from contributions by all partners. Such a process requires global averaging and local averaging. And due to governance constraints, we can’t bring all raw data together in one place.  Instead, we want to use Apache Wayang for this purpose. We simplify the procedure and split it into two phases. Phase one is the process, which allows each participant to calculate the local metrics. This requires only local data. The second phase requires data from all collaborating partners. The monthly sum and counter values per partner and category are needed in one place by all other parties. Hence, the algorithm of the first phase stores the local results locally, and the contributions to the global results in an externally accessible Kafka topic. We assume this is done by each of the partners.  Now we have a scenario, in which an Apache Wayang process must be able to read data from multiple Apache Kafka topics from multiple Apache Kafka clusters but finally writes into a single Kafka topic, which then can be accessed by all the participating clients.    The illustration shows the data flows in such a scenario. Jobs with red border are executed by the participants in isolation within their own data processing environments. But they share some of the data, using publicly accessible Kafka topics, marked by A. Job 4 is the Apache Wayang job in our focus: here we intent to read data from 3 different source systems, and write results into a fourth system (marked as B), which can be accesses by all participants again.  With this in mind we want to implement an Apache Wayang application which implements the illustrated Job 4. Since as of today, there is now KafkaSource and KafkaSink available in Apache Wayang, an implementation of both will be our first step. Our assumption is, that in the beginning, there won’t be much data.  Apache Spark is not required to cope with the load, but we expect, that in the future, a single Java application would not be able to handle our workload. Hence, we want to utilize the Apache Wayang abstraction over multiple processing platforms, starting with Java. Later, we want to switch to Apache Spark. ","version":null,"tagName":"h2"},{"title":"Apache Kafka meets Apache Wayang - Part 2","type":0,"sectionRef":"#","url":"/blog/kafka-meets-wayang-2","content":"","keywords":"","version":null},{"title":"Apache Wayang’s Read & Write Path for Kafka topics​","type":1,"pageTitle":"Apache Kafka meets Apache Wayang - Part 2","url":"/blog/kafka-meets-wayang-2#apache-wayangs-read--write-path-for-kafka-topics","content":" To describe the read and write paths for data in the context of the created Apache Wayang code snippet, the primary classes and interfaces we need to understand are as follows:  WayangContext: This class is essential for initializing the Wayang processing environment. It allows you to configure the execution environment and register plugins that define which platforms Wayang can use for data processing tasks, such as Java.basicPlugin() for local Java execution.  JavaPlanBuilder: This class is used to build and define the data processing pipeline (or plan) in Wayang. It provides a fluent API to specify the operations to be performed on the data, from reading the input to processing it and writing the output.  ","version":null,"tagName":"h2"},{"title":"Read Path​","type":1,"pageTitle":"Apache Kafka meets Apache Wayang - Part 2","url":"/blog/kafka-meets-wayang-2#read-path","content":" The read path describes how data is ingested from a source into the Wayang processing pipeline:  Reading from Kafka Topic: The method readKafkaTopic(topicName) is used to ingest data from a specified Kafka topic. This is the starting point of the data processing pipeline, where topicName represents the name of the Kafka topic from which data is read.  Data Tokenization and Preparation: Once the data is read from Kafka, it undergoes several transformations such as Splitting, Filtering, and Mapping. What follows are the procedures known as Reducing, Grouping, Co-Grouping, and Counting.  ","version":null,"tagName":"h3"},{"title":"Write Path​","type":1,"pageTitle":"Apache Kafka meets Apache Wayang - Part 2","url":"/blog/kafka-meets-wayang-2#write-path","content":" Writing to Kafka Topic: The final step in the pipeline involves writing the processed data back to a Kafka topic using .writeKafkaTopic(...). This method takes parameters that specify the target Kafka topic, a serialization function to format the data as strings, and additional configuration for load profile estimation, which optimizes the writing process.  This read-write path provides a comprehensive flow of data from ingestion from Kafka, through various processing steps, and finally back to Kafka, showcasing a full cycle of data processing within Apache Wayang's abstracted environment and is implemented in our example program shown in listing 1.  ","version":null,"tagName":"h3"},{"title":"Implementation of Input- and Output Operators​","type":1,"pageTitle":"Apache Kafka meets Apache Wayang - Part 2","url":"/blog/kafka-meets-wayang-2#implementation-of-input--and-output-operators","content":" The next section shows how a new pair of operators can be implemented to extend Apache Wayang’s capabilities on the input and output side. We created the Kafka Source and Kafka Sink components so that our cross organizational data collaboration scenario can be implemented using data streaming infrastructure.  Level 1 – Wayang execution plan with abstract operators  The implementation of our Kafka Source and Kafka Sink components for Apache Wayang requires new methods and classes on three layers. First of all in the API package. Here we use the JavaPlanBuilder to expose the function for selecting a Kafka topic as the source to be used by client. The class JavaPlanBuilder in package org.apache.wayang.api in the project wayang-api/wayang-api-scala-java exposes our new functionality to our external client. An instance of the JavaPlanBuilder is used to define the data processing pipeline. We use its readKafkaTopic() which specifies the source Kafka topic to read from, and for the write path we use the writeKafkaTopic() method. Both Methods do only trigger activities in the background.  For the output side, we use the DataQuantaBuilder class, which offers an implementation of the writeKafkaTopic function. This function is designed to send processed data, referred to as DataQuanta, to a specified Kafka topic. Essentially, it marks the final step in a data processing sequence constructed using the Apache Wayang framework.  In the DataQuanta class we implemented the methods writeKafkaTopic and writeKafkaTopicJava which use the KafkaTopicSink class. In this API layer we use the Scala programming language, but we utilize the Java classes, implemented in the layer below.  Level 2 – Wiring between Platform Abstraction and Implementation  The second layer builds the bridge between the WayangContext and PlanBuilders which work together with DataQuanta and the DataQuantaBuilder.  Also, the mapping between the abstract components and the specific implementations are defined in this layer.  Therefore, the mappings package has a class Mappings in which all relevant input and output operators are listed. We use it to register the KafkaSourceMapping and a KafkaSinkMapping for the particular platform, Java in our case. These classes allow the Apache Wayang framework to use the Java implementation of the KafkaTopicSource component (and KafkaTopicSink respectively). While the Wayang execution plan uses the higher abstractions, here on the “platform level” we have to link the specific implementation for the target platform. In our case this leads to a Java program running on a JVM which is set up by the Apache Wayang framework using the logical components of the execution plan.  Those mappings link the real implementation of our operators the ones used in an execution plan. The JavaKafkaTopicSource and the JavaKafkaTopicSink extend the KafkaTopicSource and KafkaTopicSink so that the lower level implementation of those classes become available within Wayang’s Java Platform context.  In this layer, the KafkaConsumer class and the KafkaProducer class are used, but both are configured and instantiated in the next layer underneath. All this is done in the project wayang-plarforms/wayang-java.  Layer 3 – Input/Output Connector Layer  The KafkaTopicSource and KafkaTopicSink classes build the third layer of our implementation. Both are implemented in Java programming language. In this layer, the real Kafka-Client logic is defined. Details about consumer and producers, client configuration, and schema handling have to be handled here.  ","version":null,"tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"Apache Kafka meets Apache Wayang - Part 2","url":"/blog/kafka-meets-wayang-2#summary","content":" Both classes in the third layer implement the Kafka client logic which is needed by the Wayang-execution plan when external data flows should be established. The layer above handles the mapping of the components at startup time. All this wiring is needed to keep Wayang open and flexible so that multiple external systems can be used in a variety of combinations and using multiple target platforms in combinations.  ","version":null,"tagName":"h2"},{"title":"Outlook​","type":1,"pageTitle":"Apache Kafka meets Apache Wayang - Part 2","url":"/blog/kafka-meets-wayang-2#outlook","content":" The next part of the article series will cover the creation of an Kafka Source and Sink component for the Apache Spark platform, which allows our use case to scale. Finally, in part four we bring all puzzles together, and show the full implementation of the multi organizational data collaboration use case. ","version":null,"tagName":"h2"},{"title":"Website updated","type":0,"sectionRef":"#","url":"/blog/website_update","content":"","keywords":"","version":null},{"title":"Cheatsheet​","type":1,"pageTitle":"Website updated","url":"/blog/website_update#cheatsheet","content":" List:  Line one Line one.oneLine one.two Line two Line two.oneLine two.two Line three ......  Another style for a list:  Line oneLine twoLine three ","version":null,"tagName":"h2"},{"title":"Apache Wayang vs. Presto/Trino","type":0,"sectionRef":"#","url":"/blog/wayang-vs-trino","content":"","keywords":"","version":null},{"title":"Key Distinctions​","type":1,"pageTitle":"Apache Wayang vs. Presto/Trino","url":"/blog/wayang-vs-trino#key-distinctions","content":" Trino/Presto is a query engine for distributed SQL query processing. It is composed of a coordinator and multiple workers. The coordinator consists of a query optimizer and a scheduler, while the workers are responsible for performing the necessary query processing. Data is fetched from external systems via a Connector API, i.e., Trino/Presto supports multiple data sources. Notably,query processing is is conducted exclusively by Trino/Presto workers, not the external systems.  In contrast, Wayang is a middleware for integrating diverse data platforms, including but not limited to query engines. This means that Wayang leverages the processing capabilities of the underlying data platforms to complete a given job, with no actual query processing taking place within Wayang itself.  Below you can graphically see the difference between the two systems. Note that not all available data sources or data platforms are illustrated for simplicity reasons.            I hope this makes it clear now. In fact, Trino can be easily plugged to Wayang as a platform and be seamlessly integrated with other data platforms, as shown below.    ","version":null,"tagName":"h2"},{"title":"What are the advantages of using Wayang?​","type":1,"pageTitle":"Apache Wayang vs. Presto/Trino","url":"/blog/wayang-vs-trino#what-are-the-advantages-of-using-wayang","content":" Wayang brings several benefits thanks to its integration layer:  Seamless integration of SQL query engines with ML and other data analysis systems within a single job, eliminating the need to materialize intermediate results. Users are freed from the task of specifying the query engines for an application if they desire. By submitting their Wayang job, the cross-platform optimizer can automatically determine the best data platform to use for improved performance or cost savings. Wayang facilitates cross-platform data processing by utilizing multiple data platforms to execute a query for a single job, optimizing performance and cost efficiency. Data does not have to be transferred outside their original location.  ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Apache Wayang vs. Presto/Trino","url":"/blog/wayang-vs-trino#conclusion","content":" Trino is a distributed SQL query engine which performs all the query processing of an input SQL query in a distributed manner. Wayang, on the other hand, is a data platform integrator which can automatically determine which data platform(s) is best suited for an application.  Author: zkaoudi ","version":null,"tagName":"h2"},{"title":"Becoming a committer","type":0,"sectionRef":"#","url":"/docs/community/committer","content":"","keywords":"","version":"Next"},{"title":"Sustained contributions to Wayang:​","type":1,"pageTitle":"Becoming a committer","url":"/docs/community/committer#sustained-contributions-to-wayang","content":" Committers should have a history of major contributions to Wayang. An ideal committer will have contributed broadly throughout the project, and have contributed at least one major component where they have taken an “ownership” role. An ownership role means that existing contributors feel that they should run patches for this component by this person.  Quality of contributions: Committers more than any other community member should submit simple, well-tested, and well-designed patches. In addition, they should show sufficient expertise to be able to review patches, including making sure they fit within Wayang's engineering practices (testability, documentation, API stability, code style, etc). The committership is collectively responsible for the software quality and maintainability of Spark. Note that contributions to critical parts of Spark, like its core and SQL modules, will be held to a higher standard when assessing quality. Contributors to these areas will face more review of their changes.  Community involvement: Committers should have a constructive and friendly attitude in all community interactions. They should also be active on the dev and user list and help mentor newer contributors and users. In design discussions, committers should maintain a professional and diplomatic approach, even in the face of disagreement. The type and level of contributions considered may vary by project area – for example, we greatly encourage contributors who want to work on mainly the documentation, or mainly on platform support for specific OSes, storage systems, etc.  The (P)PMC also adds new (P)PMC members. (P)PMC members are expected to carry out (P)PMC responsibilities as described in Apache Guidance, including helping vote on releases, enforce Apache project trademarks, take responsibility for legal and license issues, and ensure the project follows Apache project mechanics. The PMC periodically adds committers to the PMC who have shown they understand and can help with these activities.  ","version":"Next","tagName":"h3"},{"title":"Review process​","type":1,"pageTitle":"Becoming a committer","url":"/docs/community/committer#review-process","content":" All contributions should be reviewed before merging as described in Contributing to Wayang. In particular, if you are working on an area of the codebase you are unfamiliar with, look at the Git history for that code to see who reviewed patches before. You can do this using git log --format=full &lt;filename&gt;, by examining the “Commit” field to see who committed each patch.  ","version":"Next","tagName":"h3"},{"title":"When to commit/merge a pull request​","type":1,"pageTitle":"Becoming a committer","url":"/docs/community/committer#when-to-commitmerge-a-pull-request","content":" PRs shall not be merged during active, on-topic discussion unless they address issues such as critical security fixes of a public vulnerability. Under extenuating circumstances, PRs may be merged during active, off-topic discussion and the discussion directed to a more appropriate venue. Time should be given prior to merging for those involved with the conversation to explain if they believe they are on-topic.  Lazy consensus requires giving time for discussion to settle while understanding that people may not be working on Wayang as their full-time job and may take holidays. It is believed that by doing this, we can limit how often people feel the need to exercise their veto.  All -1s with justification merit discussion. A -1 from a non-committer can be overridden only with input from multiple committers, and suitable time must be offered for any committer to raise concerns. A -1 from a committer who cannot be reached requires a consensus vote of the (P)PMC under ASF voting rules to determine the next steps within the ASF guidelines for code vetoes. The Wayang project typically uses a 72h time window to conclude votes.  These policies serve to reiterate the core principle that code must not be merged with a pending veto or before a consensus has been reached (lazy or otherwise).  It is the (P)PMC’s hope that vetoes continue to be infrequent, and when they occur, that all parties will take the time to build consensus prior to additional feature work.  Being a committer means exercising your judgement while working in a community of people with diverse views. There is nothing wrong in getting a second (or third or fourth) opinion when you are uncertain. Thank you for your dedication to the Wayang project; it is appreciated by the developers and users of Wayang.  It is hoped that these guidelines do not slow down development; rather, by removing some of the uncertainty, the goal is to make it easier for us to reach consensus. If you have ideas on how to improve these guidelines or other Wayang project operating procedures, you should reach out on the dev@ list to start the discussion.  ","version":"Next","tagName":"h2"},{"title":"How we merge a pull request​","type":1,"pageTitle":"Becoming a committer","url":"/docs/community/committer#how-we-merge-a-pull-request","content":" Changes pushed to the master branch on Apache cannot be removed; that is, we can’t force-push to it. So please don’t add any test commits or anything like that, only real patches. We typically enforce a review from minimum two committers, who are ideally owners of submodules, like ML4ALL or JDBC. They investigate the PR, add remarks and request changes. Please be not insultet in any way, the team needs to make sure that all code committed to main has a igh quality and does not break current functionality. Ask dev@ if you have trouble with these steps, or want help doing your first merge. Once a PR is merged please leave a comment on the PR stating which branch(es) it has been merged with.  ","version":"Next","tagName":"h2"},{"title":"Policy on backporting bug fixes​","type":1,"pageTitle":"Becoming a committer","url":"/docs/community/committer#policy-on-backporting-bug-fixes","content":" We go in line with pwendell (Apache Spark PMC):  The trade off when backporting is you get to deliver the fix to people running older versions (great!), but you risk introducing new or even worse bugs in maintenance releases (bad!). The decision point is when you have a bug fix and it’s not clear whether it is worth backporting.  I think the following facets are important to consider:  Backports are an extremely valuable service to the community and should be considered for any bug fix.Introducing a new bug in a maintenance release must be avoided at all costs. It over time would erode confidence in our release process.Distributions or advanced users can always backport risky patches on their own, if they see fit.For me, the consequence of these is that we should backport in the following situations:  Both the bug and the fix are well understood and isolated. Code being modified is well tested.The bug being addressed is high priority to the community.The backported fix does not vary widely from the master branch fix.We tend to avoid backports in the converse situations:  The bug or fix are not well understood. For instance, it relates to interactions between complex components or third party libraries (e.g. Hadoop libraries). The code is not well tested outside of the immediate bug being fixed. The bug is not clearly a high priority for the community. The backported fix is widely different from the master branch fix. ","version":"Next","tagName":"h3"},{"title":"Community Hubs","type":0,"sectionRef":"#","url":"/docs/community/repositories","content":"","keywords":"","version":"Next"},{"title":"Source Code​","type":1,"pageTitle":"Community Hubs","url":"/docs/community/repositories#source-code","content":" Apache Wayang​  ASF repository : https://gitbox.apache.org/repos/asf/incubator-wayang.git GitHub mirror : https://github.com/apache/incubator-wayang.git  Wayang Web Page Repository​  ASF repository : https://gitbox.apache.org/repos/asf/incubator-wayang-website.git GitHub mirror : https://github.com/apache/incubator-wayang-website.git  ","version":"Next","tagName":"h3"},{"title":"Issues, Bugs and Features​","type":1,"pageTitle":"Community Hubs","url":"/docs/community/repositories#issues-bugs-and-features","content":" If you have a specific bug to report or feature request, we suggest opening an issue in Jira. ","version":"Next","tagName":"h3"},{"title":"Mailing List","type":0,"sectionRef":"#","url":"/docs/community/mailinglist","content":"","keywords":"","version":"Next"},{"title":"How to subscribe to a mailing list​","type":1,"pageTitle":"Mailing List","url":"/docs/community/mailinglist#how-to-subscribe-to-a-mailing-list","content":" Send an email without any contents or subject to LISTNAME-subscribe@wayang.apache.org. (replace LISTNAME with dev, commits, ..) Wait till you receive an email with the subject “confirm subscribe to LISTNAME@wayang.apache.org”. Reply to that email, without editing the subject or including any contents Wait till you receive an email with the subject “WELCOME to LISTNAME@wayang.apache.org”.  If you send us an email with a code snippet, make sure that:  you do not link to files in external services as such files can change, get deleted, or the link might break and thus make an archived email thread uselessyou paste text instead of screenshots of textyou keep formatting when pasting code in order to keep the code readablethere are enough import statements to avoid ambiguitiesif you are using some platform, please specify the version  ","version":"Next","tagName":"h2"},{"title":"Watch the email without subscribe​","type":1,"pageTitle":"Mailing List","url":"/docs/community/mailinglist#watch-the-email-without-subscribe","content":" These two channels are public mailing list, and you can get access to them on Website without subscribing.  Emails of commits@wayang.apache.orgEmails of dev@wayang.apache.org ","version":"Next","tagName":"h2"},{"title":"Adding new operators in Wayang","type":0,"sectionRef":"#","url":"/docs/guide/adding-operators","content":"","keywords":"","version":"Next"},{"title":"Step 1: Add a Wayang operator​","type":1,"pageTitle":"Adding new operators in Wayang","url":"/docs/guide/adding-operators#step-1-add-a-wayang-operator","content":" Wayang operators are located under the wayang-basic in the org.apache.wayang.basic.operators package. An operator needs to extend from one of the following abstract classes: UnaryToUnaryOperator, BinaryToUnaryOperator, UnarySource, UnarySink. For a unary to unary operator, see for example here.  For enhanced performance in Wayang, consider adding a cardinality estimator by overriding the createCardinalityEstimator() function as here.  ","version":"Next","tagName":"h2"},{"title":"Step 2: Add the (platform-specific) execution operators​","type":1,"pageTitle":"Adding new operators in Wayang","url":"/docs/guide/adding-operators#step-2-add-the-platform-specific-execution-operators","content":" Execution operators are located under the corresponding module of wayang-platforms. For instance, Java execution operators are located in the org.apache.wayang.java.operators package of the wayang-java module. An execution operator needs to extend from its corresponding Wayang operator and implement the corresponding platform operator interface. For the above MapOperator, the following is the corresponding JavaMapOperator.  For enhanced performance in Wayang, consider adding a load function as well: For this you need to overwrite the getLoadProfileEstimatorConfigurationKey() function and provide the right key that will then be read from a properties file. For the JavaMapOperator it's: wayang.java.map.load. Then add in the corresponding properties file (e.g., this is for the java executor) the template which is the mathematical formula that represents the cost of this operator and an instantiation of it. See here for the example of the map operator.  ","version":"Next","tagName":"h2"},{"title":"Step 3: Add mappings​","type":1,"pageTitle":"Adding new operators in Wayang","url":"/docs/guide/adding-operators#step-3-add-mappings","content":" Create mappings from the Wayang operator to the platform-specific execution operators. The mappings are located in the corresponding execution module in the org.apache.wayang.java.operators package. For the above MapOperator and JavaMapOperator, see here.  After that you need to declare this mapping in Wayang in the corresponding Mappings class. ","version":"Next","tagName":"h2"},{"title":"Team","type":0,"sectionRef":"#","url":"/docs/community/team","content":"","keywords":"","version":"Next"},{"title":"Committers and Contributors​","type":1,"pageTitle":"Team","url":"/docs/community/team#committers-and-contributors","content":" Name\tRole\tApache ID\tOrganizationAlexander Alten\tPPMC, Committer\taloalt\tScalytics Anis Troudi\tPPMC, Committer\tatroudi Bertty Contreras\tPPMC, Committer\tbertty\tTU Berlin Calvin Kirs\tPPMC, Committer\tkirs Jorge Quiané\tPPMC, Committer\tquiaru Rodrigo Pardo Meza\tPPMC, Committer\trpardomeza\tTU Berlin Zoi Kaoudi\tPPMC, Committer\tzkaoudi\tITU Copenhagen, Scalytics Glaucia Esppenchutz\tPPMC, Committer\tglauesppen Kaustubh Beedkar\tPPMC, Committer\tkbeedkar\tScalytics, IIT Dehli Mirko Kaempf\tContributor Ecolytiq Juri Petersen\tContributor ITU Copenhagen Mingxi Liu\tContributor East China Normal University  You can reach committers directly at &lt;apache-id&gt;@apache.org  ","version":"Next","tagName":"h2"},{"title":"Mentors during the incubation process​","type":1,"pageTitle":"Team","url":"/docs/community/team#mentors-during-the-incubation-process","content":" The following people were very kind to mentor the project while in incubation.  Name\tApache IDBernd Fondermann\tberndf Christofer Dutz\tcdutz Jean-Baptiste Onofré\tjbonofre Lars George\tlarsgeorge  ","version":"Next","tagName":"h2"},{"title":"Donations to Wayang​","type":1,"pageTitle":"Team","url":"/docs/community/team#donations-to-wayang","content":" The Logo was donated by Brian Vera ","version":"Next","tagName":"h2"},{"title":"API Documentation (WIP)","type":0,"sectionRef":"#","url":"/docs/guide/api-documentation","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"API Documentation (WIP)","url":"/docs/guide/api-documentation#overview","content":" High-level overview of the API structure  ","version":"Next","tagName":"h2"},{"title":"Classes/Modules​","type":1,"pageTitle":"API Documentation (WIP)","url":"/docs/guide/api-documentation#classesmodules","content":" Detailed documentation of classes/modules: Class/Module 1 DescriptionMethodsUsage examples  ... more classes/modules ... ","version":"Next","tagName":"h2"},{"title":"How to contribute","type":0,"sectionRef":"#","url":"/docs/community/contribute","content":"","keywords":"","version":"Next"},{"title":"Contributing by helping other users​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#contributing-by-helping-other-users","content":" A great way to contribute to Wayang is to help answer user questions on the user@ mailing list or on StackOverflow. There are always many new Wayang users; taking a few minutes to help answer a question is a very valuable community service.  Contributors should subscribe to this list and follow it in order to keep up to date on what’s happening in Wayang. Answering questions is an excellent and visible way to help the community, which also demonstrates your expertise.  See the Mailing Lists guide for guidelines about how to effectively participate in discussions on the mailing list, as well as forums like StackOverflow.  ","version":"Next","tagName":"h3"},{"title":"Contributing by testing releases​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#contributing-by-testing-releases","content":" Wayang’s release process is community-oriented, and members of the community can vote on new releases on the dev@ mailing list. Wayang users are invited to subscribe to this list to receive announcements, and test their workloads on newer release and provide feedback on any performance or correctness issues found in the newer release.  ","version":"Next","tagName":"h3"},{"title":"Contributing by reviewing changes​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#contributing-by-reviewing-changes","content":" Changes to Wayang source code are proposed, reviewed and committed via GitHub pull requests (described later). Anyone can view and comment on active changes here. Reviewing others’ changes is a good way to learn how the change process works and gain exposure to activity in various parts of the code. You can help by reviewing the changes and asking questions or pointing out issues – as simple as typos or small issues of style.  ","version":"Next","tagName":"h2"},{"title":"Contributing documentation changes​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#contributing-documentation-changes","content":" To propose a change to release documentation (that is, docs that appear under Developer section), fork the website repo and edit the Markdown source files in Wayang’s docs/ directory, the README file shows how to build the documentation locally to test your changes. The process to propose a doc change is otherwise the same as the process for proposing code changes below.  To propose a change to the rest of the documentation (that is, docs that do not appear under Developer section), similarly, edit the Markdown in the wayang-website repository and open a pull request.  ","version":"Next","tagName":"h2"},{"title":"Contributing user libraries to Wayang​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#contributing-user-libraries-to-wayang","content":" Just as Java and Scala applications can access a huge selection of libraries and utilities, none of which are part of Java or Scala themselves, Wayang aims to support a rich ecosystem of libraries. Many new useful utilities or features belong outside of Spark rather than in the core. For example: query optimizer code and language support probably has to be a part of core Wayang, but, useful machine learning algorithms can happily exist outside of Wayang.  To that end, large and independent new functionality is often rejected for inclusion in Wayang itself, but, can and should be hosted as a separate project and repository, and included in the wayang-packages.org collection.  ","version":"Next","tagName":"h2"},{"title":"Contributing bug reports​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#contributing-bug-reports","content":" Ideally, bug reports are accompanied by a proposed code change to fix the bug. This isn’t always possible, as those who discover a bug may not have the experience to fix it. A bug may be reported by creating a JIRA or a GitHub Issue but without creating a pull request (see below).  Bug reports are only useful however if they include enough information to understand, isolate and ideally reproduce the bug. Simply encountering an error does not mean a bug should be reported; as below, search JIRA and search and inquire on the Wayang user / dev mailing lists first. Unreproducible bugs, or simple error reports, may be closed.  It’s very helpful if the bug report has a description about how the bug was introduced, by which commit, so that reviewers can easily understand the bug. It also helps committers to decide how far the bug fix should be backported, when the pull request is merged. The pull request to fix the bug should narrow down the problem to the root cause.  Performance regression is also one kind of bug. The pull request to fix a performance regression must provide a benchmark to prove the problem is indeed fixed.  Note that, data correctness/data loss bugs are very serious. Make sure the corresponding bug report JIRA ticket is labeled as correctness or data-loss. If the bug report doesn’t get enough attention, please send an email to dev@, to draw more attentions.  It is possible to propose new features as well. These are generally not helpful unless accompanied by detail, such as a design document and/or code change. Large new contributions should consider wayang-packages.org first (see above), or be discussed on the mailing list first. Feature requests may be rejected, or closed after a long period of inactivity.  ","version":"Next","tagName":"h2"},{"title":"Contributing to JIRA maintenance​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#contributing-to-jira-maintenance","content":" Given the volume of issues raised in the Apache Wayang JIRA, inevitably some issues are duplicates, or become obsolete and eventually fixed otherwise, or can’t be reproduced, or could benefit from more detail, and so on. It’s useful to help identify these issues and resolve them, either by advancing the discussion or even resolving the JIRA. Most contributors are able to directly resolve JIRAs. Use judgment in determining whether you are quite confident the issue should be resolved, although changes can be easily undone. If in doubt, just leave a comment on the JIRA.  When resolving JIRAs, observe a few useful conventions:  Resolve as Fixed if there’s a change you can point to that resolved the issue Set Fix Version(s), if and only if the resolution is FixedSet Assignee to the person who most contributed to the resolution, which is usually the person who opened the PR that resolved the issue.In case several people contributed, prefer to assign to the more ‘junior’, non-committer contributor For issues that can’t be reproduced against master as reported, resolve as Cannot Reproduce Fixed is reasonable too, if it’s clear what other previous pull request resolved it. Link to it.If the issue is the same as or a subset of another issue, resolved as DuplicateMake sure to link to the JIRA it duplicatesPrefer to resolve the issue that has less activity or discussion as the duplicate If the issue seems clearly obsolete and applies to issues or components that have changed radically since it was opened, resolve as Not a Problem If the issue doesn’t make sense – not actionable, for example, a non-Spark issue, resolve as Invalid If it’s a coherent issue, but there is a clear indication that there is not support or interest in acting on it, then resolve as Won’t Fix  Sometimes, a contributor will already have a particular new change or bug in mind. If seeking ideas, consult the list of starter tasks in JIRA, or ask the user@ mailing list.  Before proceeding, contributors should evaluate if the proposed change is likely to be relevant, new and actionable:  Is it clear that code must change? Proposing a JIRA and pull request is appropriate only when a clear problem or change has been identified. If simply having trouble using Spark, use the mailing lists first, rather than consider filing a JIRA or proposing a change. When in doubt, email user@ first about the possible changeSearch the user@ and dev@ mailing list archives for related discussions. Often, the problem has been discussed before, with a resolution that doesn’t require a code change, or recording what kinds of changes will not be accepted as a resolution.Search JIRA for existing issues: https://issues.apache.org/jira/browse/WAYANGIs the scope of the change matched to the contributor’s level of experience? Anyone is qualified to suggest a typo fix, but refactoring core scheduling logic requires much more understanding of Wayang. Some changes require building up experience first (see above).It’s worth reemphasizing that changes to the core of Wayang, or to highly complex and important modules like SQL and Catalyst, are more difficult to make correctly. They will be subjected to more scrutiny, and held to a higher standard of review than changes to less critical code.  ","version":"Next","tagName":"h3"},{"title":"ML4ALL and optimizer-specific contribution guidelines​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#ml4all-and-optimizer-specific-contribution-guidelines","content":" While a rich set of algorithms is an important goal for Wayang, scaling the project requires that maintainability, consistency, and code quality come first. New algorithms should:  Be used and accepted (academic citations and concrete use cases can help justify this)Be well documentedHave APIs consistent with other algorithms in WayangCome with a reasonable expectation of developer supportError message guidelinesExceptions thrown in Wayang should be associated with standardized and actionable error messages  Error messages should answer the following questions:​  What was the problem?Why did the problem happen?How can the problem be solved?  When writing error messages, you should:  Use active voiceAvoid time-based statements, such as promises of future supportUse the present tense to describe the error and provide suggestionsProvide concrete examples if the resolution is unclearAvoid sounding accusatory, judgmental, or insultingBe directDo not use programming jargon in user-facing errors 8 See the error message guidelines for more details.  ","version":"Next","tagName":"h3"},{"title":"Code review criteria​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#code-review-criteria","content":" Before considering how to contribute code, it’s useful to understand how code is reviewed, and why changes may be rejected. See the detailed guide for code reviewers from Google’s Engineering Practices documentation. Simply put, changes that have many or large positives, and few negative effects or risks, are much more likely to be merged, and merged quickly. Risky and less valuable changes are very unlikely to be merged, and may be rejected outright rather than receive iterations of review.  Positives​  Fixes the root cause of a bug in existing functionalityAdds functionality or fixes a problem needed by a large number of usersSimple, targetedMaintains or improves consistency across Python, Java, ScalaEasily tested; has testsReduces complexity and lines of codeChange has already been discussed and is known to committers  Negatives, risks​  Band-aids a symptom of a bug onlyIntroduces complex new functionality, especially an API that needs to be supportedAdds complexity that only helps a niche use caseAdds user-space functionality that does not need to be maintained in Wayang, but could be hosted externally and indexed by wayang-packages.orgChanges a public API or semantics (rarely allowed)Adds large dependenciesChanges versions of existing dependenciesAdds a large amount of codeMakes lots of modifications in one “big bang” change  ","version":"Next","tagName":"h3"},{"title":"Contributing code changes​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#contributing-code-changes","content":" Please review the preceding section before proposing a code change. This section documents how to do so.  When you contribute code, you affirm that the contribution is your original work and that you license the work to the project under the project’s open source license. Whether or not you state this explicitly, by submitting any copyrighted material via pull request, email, or other means you agree to license the material under the project’s open source license and warrant that you have the legal authority to do so.  ","version":"Next","tagName":"h3"},{"title":"Cloning the Apache Wayang™ source code​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#cloning-the-apache-wayang-source-code","content":" If you are interested in working with the newest under-development code or contributing to Apache Spark development, you can check out the master branch from Git:  # Master development branch git clone git://github.com/apache/incubator-wayang.git   Once you’ve downloaded Wayang, you can find instructions for installing and building it on the Compiling Apache Wayang page.  ","version":"Next","tagName":"h3"},{"title":"JIRA​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#jira","content":" Generally, Wayang uses JIRA to track logical issues, including bugs and improvements, and uses GitHub pull requests to manage the review and merge of specific code changes. That is, JIRAs are used to describe what should be fixed or changed, and high-level approaches, and pull requests describe how to implement that change in the project’s source code. For example, major design decisions are discussed in JIRA.  Find the existing Spark JIRA that the change pertains to. Do not create a new JIRA if creating a change to address an existing issue in JIRA; add to the existing discussion and work insteadLook for existing pull requests that are linked from the JIRA, to understand if someone is already working on the JIRA If the change is new, then it usually needs a new JIRA. However, trivial changes, where the what should change is virtually the same as the how it should change do not require a JIRA. Example: Fix typos in Foo wayang docIf required, create a new JIRA: Provide a descriptive Title. “Update web UI” or “Problem in scheduler” is not sufficient. “Kafka Streaming support fails to handle empty queue in YARN cluster mode” is good.Write a detailed description. For bug reports, this should ideally include a short reproduction of the problem. For new features, it may include a design document.Set required fields: Issue Type. Generally, Bug, Improvement and New Feature are the only types used in Spark.Priority. Set to Major or below; higher priorities are generally reserved for committers to set. The main exception is correctness or data-loss issues, which can be flagged as Blockers. JIRA tends to unfortunately conflate “size” and “importance” in its Priority field values. Their meaning is roughly: Blocker: pointless to release without this change as the release would be unusable to a large minority of users. Correctness and data loss issues should be considered Blockers for their target versions.Critical: a large minority of users are missing important functionality without this, and/or a workaround is difficultMajor: a small minority of users are missing important functionality without this, and there is a workaroundMinor: a niche use case is missing some support, but it does not affect usage or is easily worked aroundTrivial: a nice-to-have change but unlikely to be any problem in practice otherwise ComponentAffects Version. For Bugs, assign at least one version that is known to exhibit the problem or need the changeLabel. Not widely used, except for the following: correctness: a correctness issuedata-loss: a data loss issuerelease-notes: the change’s effects need mention in release notes. The JIRA or pull request should include detail suitable for inclusion in release notes – see “Docs Text” below.starter: small, simple change suitable for new contributors Docs Text: For issues that require an entry in the release notes, this should contain the information that the release manager should include in Release Notes. This should include a short summary of what behavior is impacted, and detail on what behavior changed. It can be provisionally filled out when the JIRA is opened, but will likely need to be updated with final details when the issue is resolved. Do not set the following fields: Fix Version This is assigned by committers only when resolved.Target Version This is assigned by committers to indicate a PR has been accepted for possible fix by the target version. Do not include a patch file; pull requests are used to propose the actual change. If the change is a large change, consider inviting discussion on the issue at dev@ first before proceeding to implement the change.  ","version":"Next","tagName":"h3"},{"title":"Pull request​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#pull-request","content":" Before creating a pull request in Apache Wayang, it is important to check if tests can pass on your branch because our GitHub Actions workflows automatically run tests for your pull request/following commits and every run burdens the limited resources of GitHub Actions in Apache Wayang repository. Below steps will take your through the process.  Fork the GitHub repository at https://github.com/apache/incubator-wayang if you haven’t alreadyGo to “Actions” tab on your forked repository and enable “Build and test” and “Report test results” workflowsClone your fork and create a new branchConsider whether documentation or tests need to be added or updated as part of the change, and add them as needed. When you add tests, make sure the tests are self-descriptive.Also, you should consider writing a JIRA ID in the tests when your pull request targets to fix a specific issue. In practice, usually it is added when a JIRA type is a bug or a PR adds a couple of tests to an existing test class. See the examples below:  @Test public void testCase() { // WAYANG-12345: a short description of the test   Consider whether benchmark results should be added or updated as part of the change, and add them as needed by Running benchmarks in your forked repository to generate benchmark results.Run all tests with in your build to verify that the code still compiles, passes tests, and passes style checks. If style checks fail, review the Code Style Guide below.Push commits to your branch. This will trigger “Build and test” and “Report test results” workflows on your forked repository and start testing and validating your changes.Open a pull request against the master branch of apache/incubator-wayang. (Only in special cases would the PR be opened against other branches). This will trigger workflows “On pull request*” (on the Wayang repo) that will look/watch for successful workflow runs on “your” forked repository (it will wait if one is running). The PR title should be of the form [WAYANG-xxxx][COMPONENT] Title, where WAYANG-xxxx is the relevant JIRA number, COMPONENT is one of the PR categories and Title may be the JIRA’s title or a more specific title describing the PR itself.If the pull request is still a work in progress, and so is not ready to be merged, but needs to be pushed to GitHub to facilitate review, then add [WIP] after the component.Consider identifying committers or other contributors who have worked on the code being changed. Find the file(s) in GitHub and click “Blame” to see a line-by-line annotation of who changed the code last. You can add @username in the PR description to ping them immediately.Please state that the contribution is your original work and that you license the work to the project under the project’s open source license. The related JIRA, if any, will be marked as “In Progress” and your pull request will automatically be linked to it. There is no need to be the Assignee of the JIRA to work on it, though you are welcome to comment that you have begun work.  ","version":"Next","tagName":"h3"},{"title":"The review process​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#the-review-process","content":" Other reviewers, including committers, may comment on the changes and suggest modifications. Changes can be added by simply pushing more commits to the same branch.Lively, polite, rapid technical debate is encouraged from everyone in the community. The outcome may be a rejection of the entire change.Keep in mind that changes to more critical parts of Spark, like its core and SQL components, will be subjected to more review, and may require more testing and proof of its correctness than other changes.Reviewers can indicate that a change looks suitable for merging with a comment such as: “I think this patch looks good”. Wayang uses the LGTM convention for indicating the strongest level of technical sign-off on a patch: simply comment with the word “LGTM”. It specifically means: “I’ve looked at this thoroughly and take as much ownership as if I wrote the patch myself”. If you comment LGTM you will be expected to help with bugs or follow-up issues on the patch. Consistent, judicious use of LGTMs is a great way to gain credibility as a reviewer with the broader community.Sometimes, other changes will be merged which conflict with your pull request’s changes. The PR can’t be merged until the conflict is resolved. This can be resolved by, for example, adding a remote to keep up with upstream changes by git remote add upstream https://github.com/apache/incubator-wayang.git, running git fetch upstream followed by git rebase upstream/master and resolving the conflicts by hand, then pushing the result to your branch.Try to be responsive to the discussion rather than let days pass between replies  ","version":"Next","tagName":"h3"},{"title":"Closing your pull request / JIRA​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#closing-your-pull-request--jira","content":" If a change is accepted, it will be merged and the pull request will automatically be closed, along with the associated JIRA if any Note that in the rare case you are asked to open a pull request against a branch besides master, that you will actually have to close the pull request manuallyThe JIRA will be Assigned to the primary contributor to the change as a way of giving credit. If the JIRA isn’t closed and/or Assigned promptly, comment on the JIRA. If your pull request is ultimately rejected, please close it promptly … because committers can’t close PRs directlyPull requests will be automatically closed by an automated process at Apache after about a week if a committer has made a comment like “mind closing this PR?” This means that the committer is specifically requesting that it be closed. If a pull request has gotten little or no attention, consider improving the description or the change itself and ping likely reviewers again after a few days. Consider proposing a change that’s easier to include, like a smaller and/or less invasive change.If it has been reviewed but not taken up after weeks, after soliciting review from the most relevant reviewers, or, has met with neutral reactions, the outcome may be considered a “soft no”. It is helpful to withdraw and close the PR in this case.If a pull request is closed because it is deemed not the right approach to resolve a JIRA, then leave the JIRA open. However if the review makes it clear that the issue identified in the JIRA is not going to be resolved by any pull request (not a problem, won’t fix) then also resolve the JIRA.  ","version":"Next","tagName":"h3"},{"title":"Code style guide​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#code-style-guide","content":" Please follow the style of the existing codebase.  For Python code, Apache Wayang follows PEP 8 with one exception: lines can be up to 100 characters in length, not 79.For R code, Apache Wayang follows Google’s R Style Guide with three exceptions: lines can be up to 100 characters in length, not 80, there is no limit on function name but it has a initial lower case latter and S4 objects/methods are allowed.For Java code, Apache Wayang follows Oracle’s Java code conventions and Scala guidelines below. The latter is preferred.For Scala code, Apache Wayang follows the official Scala style guide.  ","version":"Next","tagName":"h2"},{"title":"If in doubt​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#if-in-doubt","content":" If you’re not sure about the right style for something, try to follow the style of the existing codebase. Look at whether there are other examples in the code that use your feature. Feel free to ask on the dev@ list as well and/or ask committers.  ","version":"Next","tagName":"h3"},{"title":"Code of conduct​","type":1,"pageTitle":"How to contribute","url":"/docs/community/contribute#code-of-conduct","content":" The Apache Wayang project follows the Apache Software Foundation Code of Conduct. The code of conduct applies to all spaces managed by the Apache Software Foundation, including IRC, all public and private mailing lists, issue trackers, wikis, blogs, Twitter, and any other communication channel used by our communities. A code of conduct which is specific to in-person events (ie., conferences) is codified in the published ASF anti-harassment policy.  We expect this code of conduct to be honored by everyone who participates in the Apache community formally or informally, or claims any affiliation with the Foundation, in any Foundation-related activities and especially when representing the ASF, in any role.  This code is not exhaustive or complete. It serves to distill our common understanding of a collaborative, shared environment and goals. We expect it to be followed in spirit as much as in the letter, so that it can enrich all of us and the technical communities in which we participate.  For more information and specific guidelines, refer to the Apache Software Foundation Code of Conduct.  This guide was originally released by Apache Spark, the Apache Wayang project adapted the guide. ","version":"Next","tagName":"h2"},{"title":"Developing in Wayang","type":0,"sectionRef":"#","url":"/docs/guide/developing-in-wayang","content":"","keywords":"","version":"Next"},{"title":"Compile the module you modified​","type":1,"pageTitle":"Developing in Wayang","url":"/docs/guide/developing-in-wayang#compile-the-module-you-modified","content":" Within the root directory of Wayang, compile only the module you modified for faster compilation:  mvn clean install -DskipTests -pl &lt;modified_module&gt;   or change the directory to your module and compile there:  cd &lt;modified_module&gt; &amp;&amp; mvn clean install   Important: before making a Pull Request make sure all modules compile and all tests are passing:  mvn clean install   ","version":"Next","tagName":"h2"},{"title":"Package the project​","type":1,"pageTitle":"Developing in Wayang","url":"/docs/guide/developing-in-wayang#package-the-project","content":" mvn clean package -pl :wayang-assembly -Pdistribution   ","version":"Next","tagName":"h2"},{"title":"Execute your code​","type":1,"pageTitle":"Developing in Wayang","url":"/docs/guide/developing-in-wayang#execute-your-code","content":" Before executing your code, make sure the required environment variables are set correctly.  cd wayang-assembly/target/ tar -xvf apache-wayang-assembly-0.7.1-SNAPSHOT-incubating-dist.tar.gz cd wayang-0.7.1-SNAPSHOT ./bin/wayang-submit org.apache.wayang.&lt;main_class&gt; &lt;parameters&gt;  ","version":"Next","tagName":"h2"},{"title":"ML4all: scalable ML system for everyone","type":0,"sectionRef":"#","url":"/docs/guide/ml4all","content":"","keywords":"","version":"Next"},{"title":"Abstraction​","type":1,"pageTitle":"ML4all: scalable ML system for everyone","url":"/docs/guide/ml4all#abstraction","content":" ML4all abstracts most ML algorithms with seven operators:  (1) Transform receives a data point to transform (e.g., normalize it) and outputs a new data point. (2) Stage initializes all the required global param- eters (e.g., centroids for the k-means algorithm). (3) Compute performs user-defined computations on the input data point and returns a new data point. For example, it can compute the nearest cen- troid for each input data point. (4) Update updates the global parameters based on a user-defined formula. For example, it can update the new centroids based on the output computed by the Compute operator. (5) Sample takes as input the size of the desired sample and the data points to sample from and re- turns a reduced set of sampled data points. (6) Converge specifies a function that outputs a convergence dataset required for determining whether the iterations should continue or stop. (7) Loop specifies the stopping condition on the convergence dataset.  Similar to MapReduce, where users need to implement a map and reduce function, users of ML4all wishing to develop their own algorithm should implement the above interfaces. The interfaces can be found in org.apache.wayang.ml4all.abstraction.api.  Examples for KMeans clustering and stochastic gradient descent can be found in org.apache.wayang.ml4all.algorithms.  ","version":"Next","tagName":"h2"},{"title":"Example runs​","type":1,"pageTitle":"ML4all: scalable ML system for everyone","url":"/docs/guide/ml4all#example-runs","content":" Kmeans:  ./bin/wayang-submit org.apache.wayang.ml4all.examples.RunKMeans java,spark &lt;url_path_to_file&gt;/USCensus1990-sample.input 3 68 0 1   SGD:  ./bin/wayang-submit org.apache.wayang.ml4all.examples.RunSGD spark &lt;url_path_to_file&gt;/adult.zeros.input 100827 123 10 0.001  ","version":"Next","tagName":"h2"},{"title":"Installing and building Apache Wayang","type":0,"sectionRef":"#","url":"/docs/guide/installation","content":"","keywords":"","version":"Next"},{"title":"Clone repository​","type":1,"pageTitle":"Installing and building Apache Wayang","url":"/docs/guide/installation#clone-repository","content":" git clone https://github.com/apache/incubator-wayang.git   ","version":"Next","tagName":"h2"},{"title":"Create binaries​","type":1,"pageTitle":"Installing and building Apache Wayang","url":"/docs/guide/installation#create-binaries","content":" Running following commands to build Wayang and generate the tar.gz  cd incubator-wayang ./mvnw clean package -pl :wayang-assembly -Pdistribution   Then you can find the wayang-assembly-0.7.1-SNAPSHOT-dist.tar.gz under wayang-assembly/target directory.  ","version":"Next","tagName":"h2"},{"title":"Prepare the environment​","type":1,"pageTitle":"Installing and building Apache Wayang","url":"/docs/guide/installation#prepare-the-environment","content":" ","version":"Next","tagName":"h2"},{"title":"Wayang​","type":1,"pageTitle":"Installing and building Apache Wayang","url":"/docs/guide/installation#wayang","content":" tar -xvf wayang-assembly-0.7.1-SNAPSHOT-dist.tar.gz cd wayang-0.7.1-SNAPSHOT   In linux  echo &quot;export WAYANG_HOME=$(pwd)&quot; &gt;&gt; ~/.bashrc echo &quot;export PATH=${PATH}:${WAYANG_HOME}/bin&quot; &gt;&gt; ~/.bashrc source ~/.bashrc   In MacOS  echo &quot;export WAYANG_HOME=$(pwd)&quot; &gt;&gt; ~/.zshrc echo &quot;export PATH=${PATH}:${WAYANG_HOME}/bin&quot; &gt;&gt; ~/.zshrc source ~/.zshrc   ","version":"Next","tagName":"h3"},{"title":"Others​","type":1,"pageTitle":"Installing and building Apache Wayang","url":"/docs/guide/installation#others","content":" You need to install Apache Spark version 3 or higher. Don’t forget to set the SPARK_HOME environment variable.You need to install Apache Hadoop version 3 or higher. Don’t forget to set the HADOOP_HOME environment variable.  ","version":"Next","tagName":"h3"},{"title":"Run the program​","type":1,"pageTitle":"Installing and building Apache Wayang","url":"/docs/guide/installation#run-the-program","content":" To execute the WordCount example with Apache Wayang, you need to execute your program with the 'wayang-submit' command:  cd wayang-0.7.1-SNAPSHOT ./bin/wayang-submit org.apache.wayang.apps.wordcount.Main java file://$(pwd)/README.md   Then you should be able to see the output of the Wordcount example.  Compiling Apache Wayang  Apache Wayang (incubating) has different dependencies, for compiling, it needs to add some profile in the compilation to enable maven works properly.  mvn clean compile   The line before is because the plugin the Antlr is not needed in all the modules, as well it has happened with Scala language.  When maven compiles one or more modules using those plugins in the compilation time, it needs to add.  The modules are:  wayang-api-scala-javawayang-core (Antlr)wayang-iejoinwayang-sparkwayang-profilerwayang-tests-integration  Executing Coverage  mvn clean verify jacoco:report   the final report is placed on ./target/aggregate.exec/aggregate.exec ","version":"Next","tagName":"h2"},{"title":"How the optimizer works (WIP)","type":0,"sectionRef":"#","url":"/docs/guide/optimizer","content":"","keywords":"","version":"Next"},{"title":"Configuration Files​","type":1,"pageTitle":"How the optimizer works (WIP)","url":"/docs/guide/optimizer#configuration-files","content":" Description of configuration files and how to edit them  ","version":"Next","tagName":"h2"},{"title":"Key Configuration Parameters​","type":1,"pageTitle":"How the optimizer works (WIP)","url":"/docs/guide/optimizer#key-configuration-parameters","content":" List of key parameters and how they affect performance  ","version":"Next","tagName":"h2"},{"title":"Tuning Tips​","type":1,"pageTitle":"How the optimizer works (WIP)","url":"/docs/guide/optimizer#tuning-tips","content":" Tips on tuning Apache Wayang for different scenarios ","version":"Next","tagName":"h2"},{"title":"Guide to Development and Usage with Apache Wayang (incubating)","type":0,"sectionRef":"#","url":"/docs/guide/examples","content":"","keywords":"","version":"Next"},{"title":"Example 1: Machine Learning for query optimization in Apache Wayang​","type":1,"pageTitle":"Guide to Development and Usage with Apache Wayang (incubating)","url":"/docs/guide/examples#example-1-machine-learning-for-query-optimization-in-apache-wayang","content":" Apache Wayang can be customized with concrete implementations of the EstimatableCost interface in order to optimize for a desired metric. The implementation can be enabled by providing it to a Configuration.  public class CustomEstimatableCost implements EstimatableCost { /* Provide concrete implementations to match desired cost function(s) * by implementing the interface in this class. */ } public class WordCount { public static void main(String[] args) { /* Create a Wayang context and specify the platforms Wayang will consider */ Configuration config = new Configuration(); /* Provision of a EstimatableCost that implements the interface.*/ config.setCostModel(new CustomEstimatableCost()); WayangContext wayangContext = new WayangContext(config) .withPlugin(Java.basicPlugin()) .withPlugin(Spark.basicPlugin()); /*... omitted */ } }   In combination with an encoding scheme and a third party package to load ML models, the following example shows how to predict runtimes of query execution plans runtimes in Apache Wayang (incubating):  public class MLCost implements EstimatableCost { public EstimatableCostFactory getFactory() { return new Factory(); } public static class Factory implements EstimatableCostFactory { @Override public EstimatableCost makeCost() { return new MLCost(); } } @Override public ProbabilisticDoubleInterval getEstimate(PlanImplementation plan, boolean isOverheadIncluded) { try { Configuration config = plan .getOptimizationContext() .getConfiguration(); OrtMLModel model = OrtMLModel.getInstance(config); return ProbabilisticDoubleInterval.ofExactly( model.runModel(OneHotEncoder.encode(plan)) ); } catch(Exception e) { return ProbabilisticDoubleInterval.zero; } } @Override public ProbabilisticDoubleInterval getParallelEstimate(PlanImplementation plan, boolean isOverheadIncluded) { try { Configuration config = plan .getOptimizationContext() .getConfiguration(); OrtMLModel model = OrtMLModel.getInstance(config); return ProbabilisticDoubleInterval.ofExactly( model.runModel(OneHotEncoder.encode(plan)) ); } catch(Exception e) { return ProbabilisticDoubleInterval.zero; } } /** Returns a squashed cost estimate. */ @Override public double getSquashedEstimate(PlanImplementation plan, boolean isOverheadIncluded) { try { Configuration config = plan .getOptimizationContext() .getConfiguration(); OrtMLModel model = OrtMLModel.getInstance(config); return model.runModel(OneHotEncoder.encode(plan)); } catch(Exception e) { return 0; } } @Override public double getSquashedParallelEstimate(PlanImplementation plan, boolean isOverheadIncluded) { try { Configuration config = plan .getOptimizationContext() .getConfiguration(); OrtMLModel model = OrtMLModel.getInstance(config); return model.runModel(OneHotEncoder.encode(plan)); } catch(Exception e) { return 0; } } @Override public Tuple&lt;List&lt;ProbabilisticDoubleInterval&gt;, List&lt;Double&gt;&gt; getParallelOperatorJunctionAllCostEstimate(PlanImplementation plan, Operator operator) { List&lt;ProbabilisticDoubleInterval&gt; intervalList = new ArrayList&lt;ProbabilisticDoubleInterval&gt;(); List&lt;Double&gt; doubleList = new ArrayList&lt;Double&gt;(); intervalList.add(this.getEstimate(plan, true)); doubleList.add(this.getSquashedEstimate(plan, true)); return new Tuple&lt;&gt;(intervalList, doubleList); } public PlanImplementation pickBestExecutionPlan( Collection&lt;PlanImplementation&gt; executionPlans, ExecutionPlan existingPlan, Set&lt;Channel&gt; openChannels, Set&lt;ExecutionStage&gt; executedStages) { final PlanImplementation bestPlanImplementation = executionPlans.stream() .reduce((p1, p2) -&gt; { final double t1 = p1.getSquashedCostEstimate(); final double t2 = p2.getSquashedCostEstimate(); return t1 &lt; t2 ? p1 : p2; }) .orElseThrow(() -&gt; new WayangException(&quot;Could not find an execution plan.&quot;)); return bestPlanImplementation; } }   Third-party packages such as OnnxRuntime can be used to load pre-trained .onnx files that contain desired ML models.  public class OrtMLModel { private static OrtMLModel INSTANCE; private OrtSession session; private OrtEnvironment env; private final Map&lt;String, OnnxTensor&gt; inputMap = new HashMap&lt;&gt;(); private final Set&lt;String&gt; requestedOutputs = new HashSet&lt;&gt;(); public static OrtMLModel getInstance(Configuration configuration) throws OrtException { if (INSTANCE == null) { INSTANCE = new OrtMLModel(configuration); } return INSTANCE; } private OrtMLModel(Configuration configuration) throws OrtException { this.loadModel(configuration.getStringProperty(&quot;wayang.ml.model.file&quot;)); } public void loadModel(String filePath) throws OrtException { if (this.env == null) { this.env = OrtEnvironment.getEnvironment(); } if (this.session == null) { this.session = env.createSession(filePath, new OrtSession.SessionOptions()); } } public void closeSession() throws OrtException { this.session.close(); this.env.close(); } /** * @param encodedVector * @return NaN on error, and a predicted cost on any other value. * @throws OrtException */ public double runModel(Vector&lt;Long&gt; encodedVector) throws OrtException { double costPrediction; OnnxTensor tensor = OnnxTensor.createTensor(env, encodedVector); this.inputMap.put(&quot;input&quot;, tensor); this.requestedOutputs.add(&quot;output&quot;); BiFunction&lt;Result, String, Double&gt; unwrapFunc = (r, s) -&gt; { try { return ((double[]) r.get(s).get().getValue())[0]; } catch (OrtException e) { return Double.NaN; } }; try (Result r = session.run(inputMap, requestedOutputs)) { costPrediction = unwrapFunc.apply(r, &quot;output&quot;); } return costPrediction; } }  ","version":"Next","tagName":"h2"},{"title":"What is Wayang?","type":0,"sectionRef":"#","url":"/docs/introduction/about","content":"","keywords":"","version":"Next"},{"title":"Benchmarking Wayang","type":0,"sectionRef":"#","url":"/docs/introduction/benchmark","content":"","keywords":"","version":"Next"},{"title":"Running a Relational Query Across Multiple Independent Databases (Federation)​","type":1,"pageTitle":"Benchmarking Wayang","url":"/docs/introduction/benchmark#running-a-relational-query-across-multiple-independent-databases-federation","content":" In this use case, we showcase how Apache Wayang addresses the common challenge of analyzing data scattered across diverse systems, a prevalent issue in many data-driven projects. Through Wayang, developers can seamlessly perform analytics on relational data residing in multiple systems. The in-situ data processing approach employed by Wayang introduces data privacy-preserving mechanisms into distributed data analytics tasks.  Datasets: To test the capabilities of Wayang, we utilized the TPC-H benchmark, which comprises five datasets/relations. The lineitem and orders relations are stored in HDFS, while customer, supplier, and region are in Postgres, and nation is in S3 or the local file system. Our testing covered a range of dataset sizes, from 1GB to 100GB, to ensure our product's scalability and robustness.  Query/task: We evaluate the performance of the complex SQL TPC-H query 5:  SELECT N_NAME, SUM(L_EXTENDEDPRICE*(1-L_DISCOUNT)) AS REVENUE FROM CUSTOMER, ORDERS, LINEITEM, SUPPLIER, NATION, REGION WHERE C_CUSTKEY = O_CUSTKEY AND L_ORDERKEY = O_ORDERKEY AND L_SUPPKEY = S_SUPPKEY AND C_NATIONKEY = S_NATIONKEY AND S_NATIONKEY = N_NATIONKEY AND N_REGIONKEY = R_REGIONKEY AND R_NAME = 'ASIA' AND O_ORDERDATE &gt;= '1994-01-01' AND O_ORDERDATE &lt; DATEADD(YY, 1, cast('1994-01-01' as date)) GROUP BY N_NAME ORDER BY REVENUE DESC   Baselines: We compared Wayang with two widely-used systems for storing and querying relational data: Spark and Postgres. To conduct a fair comparison, we first moved all datasets to the system being tested (Spark or Postgres).  Results: The results of our tests, presented in Figure 1, show the execution time in seconds of the SQL query 5. Note that the time required to transfer data to Spark or Postgres was excluded from the results. As seen in Figure 1, Wayang significantly outperformed Postgres while retaining a runtime very close to Spark. It's important to note that for Spark to function, we needed twice the time to extract datasets from Postgres and transfer them to Spark. Wayang achieved such impressive performance by seamlessly combining Postgres with Spark: Wayang's query optimizer chose to perform all selections and projections on the data stored in Postgres before extracting data to join with the relations in HDFS. Additionally, the optimizer determined that executing the join between lineitem and supplier in Spark would be beneficial, as it could distribute the computational load of joining to multiple workers. All of this was done without the need for the user to specify where each operation should be executed.      ","version":"Next","tagName":"h3"},{"title":"Reducing Execution Costs for Machine Learning Tasks Using Multiple Systems​","type":1,"pageTitle":"Benchmarking Wayang","url":"/docs/introduction/benchmark#reducing-execution-costs-for-machine-learning-tasks-using-multiple-systems","content":" To evaluate Apache Wayang's ability to accelerate machine learning tasks while optimizing costs by leveraging multiple systems, we conducted a comprehensive benchmark. The benchmark involved storing all data within a single repository, a scenario where selectively offloading data to more powerful engines can significantly enhance performance. To assess this feature, we employed a widely used machine learning algorithm, stochastic gradient descent, to perform classification tasks.   Datasets: We use two real-world datasets that we downloaded from the UCI machine learning repository, namely higgs and rcv1. higgs consists of ~11 million data points with 28 features each and rcv1 contains ~677 thousand data points with ~47 thousand features each. In addition, we construct a synthetic dataset so that we can stress our product with even larger datasets, specifically with 88 million data points of 100 features each.  Query/task: We test the performance of training classification models for higgs and the synthetic dataset and a regression model rcv1. All three training models use stochastic gradient descent but with a different loss function. We use Hinge loss to simulate support vector machines for the classification tasks and the logistic loss for the regression task.  Baselines: We compare Wayang against MLlib from Apache Spark and SystemML from IBM, two very popular machine learning libraries.  Results: Our results, shown in Figure 2, demonstrate that Wayang outperforms both baselines by more than an order of magnitude in runtime performance for large datasets.      Apache Wayang's intelligent optimizer dynamically utilizes a hybrid approach, seamlessly blending Spark and local Java execution, to significantly improve the performance of the stochastic gradient descent algorithm. This optimization strategy involves leveraging Spark for efficient preprocessing and data preparation, while transitioning to local Java execution for the subsequent gradient computation phase. During this phase, the dataset shrinks considerably, making it more efficient to process using a single machine. This optimization technique, while not readily apparent without specialized expertise, is seamlessly implemented by Apache Wayang, requiring no additional user intervention.  ","version":"Next","tagName":"h3"},{"title":"Optimizing Big Data Analytics by Adapting Platforms to Data and Task Characteristics​","type":1,"pageTitle":"Benchmarking Wayang","url":"/docs/introduction/benchmark#optimizing-big-data-analytics-by-adapting-platforms-to-data-and-task-characteristics","content":" We demonstrate the platform adaptation capabilities of Apache Wayang, showcasing how it optimizes analytical tasks by tailoring execution to the specific data and task characteristics. Our approach involves dynamically selecting the optimal processing engine based on the nature of the data and the complexity of the analytical operation. This flexibility allows Wayang to deliver enhanced performance and efficiency, particularly for complex analytical workflows that demand the strengths of multiple processing engines.  Datasets: We use Wikipedia abstracts and store them in HDFS. We vary the dataset size from 1GB to 800GB.  Query/task: We test our product with a widely popular big data analytical task, namely WordCount. It counts the number of distinct words in a text corpus. Different variations of wordcount are useful in various text mining applications, such as term frequency, word clouds, etc.  Baselines: We use three different platforms where Wordcount can be executed: Apache Spark, Apache Flink, and a single node Java program. We then set Wayang to be able to automatically choose which of the three platforms to use for each dataset.  Results: The benchmark results, presented in Figure 3, provide evidence of Apache Wayang's platform adaptation prowess, consistently selecting the fastest available platform for various dataset sizes in our WordCount example. By incorporating execution cost modeling into the query optimizer, Wayang eliminates the need for manual platform selection or migration, allowing users to solely focus on their analytical tasks. Our platform adaptation feature seamlessly identifies the optimal platform for each query or task based on data and query characteristics, ensuring faster execution and enhanced efficiency in big data analytics.         ","version":"Next","tagName":"h3"},{"title":"Features","type":0,"sectionRef":"#","url":"/docs/introduction/features","content":"","keywords":"","version":"Next"},{"title":"Cross platform enablement​","type":1,"pageTitle":"Features","url":"/docs/introduction/features#cross-platform-enablement","content":" Apache Wayang's cross-platform optimizer stands out as its most distinguishing feature. It not only selects the most suitable processing engine for each task, but also empowers users to execute a single task across multiple platforms. This capability is achieved through an extensible set of graph transformations applied to the Apache Wayang plan, which identifies alternative execution plans that potentially offer better performance. These alternative plans are then evaluated using platform-specific cost models, which can either be user-defined or learned from historical data. These cost models are parameterized based on the underlying hardware configuration, such as the number of computing nodes for distributed operations.  ","version":"Next","tagName":"h3"},{"title":"Scalable ML (ML4All)​","type":1,"pageTitle":"Features","url":"/docs/introduction/features#scalable-ml-ml4all","content":" ML4all is part of Wayang, enhancing machine learning workflows. ML4all alleviates users from the complexities of algorithm selection and low-level implementation details, enabling them to concentrate on the core aspects of their machine learning applications. By employing an innovative abstraction, ML4all addresses a broad spectrum of machine learning tasks and utilizes a cost-based optimizer to dynamically identify the optimal gradient descent algorithm for each scenario. Notably, ML4all demonstrates performance improvements of two orders of magnitude over existing systems, enabling the processing of large datasets that were previously impractical.  ","version":"Next","tagName":"h3"},{"title":"High Efficiency​","type":1,"pageTitle":"Features","url":"/docs/introduction/features#high-efficiency","content":" Apache Wayang's suite of optimized operators and sophisticated query optimization techniques enables it to effectively process large-scale and small-scale datasets. By employing a data processing abstraction based on UDFs, Wayang allows applications to convey semantic information about their functions, optimization hints (such as iteration counts), constraints (e.g., physical placement of operators), and alternative execution plans. The optimizer incorporates these insights to refine the execution plan, aiming to achieve optimal performance in a comprehensive manner.  ","version":"Next","tagName":"h3"},{"title":"Flexibility​","type":1,"pageTitle":"Features","url":"/docs/introduction/features#flexibility","content":" Apache Wayang offers a collection of operators that applications utilize to define their tasks. A crucial aspect of Apache Wayang is its flexible operator mapping structure, which enables developers to effortlessly add, modify, or remove mappings between Wayang and execution operators. This flexibility empowers developers to seamlessly integrate additional Wayang and execution operators into their applications.  ","version":"Next","tagName":"h3"},{"title":"Cost Saving​","type":1,"pageTitle":"Features","url":"/docs/introduction/features#cost-saving","content":" Developers can focus on building their applications without the need to understand the complexities of underlying platforms. This simplifies the development process and removes the requirement for developers to be experts in big data infrastructures. Apache Wayang automatically determines the most suitable data processing platforms for specific tasks and deploys applications accordingly.  ","version":"Next","tagName":"h3"},{"title":"Additonal Features​","type":1,"pageTitle":"Features","url":"/docs/introduction/features#additonal-features","content":" Zero-copy: cross-platform in-situ data processingHigh performance: A highly-extensible API framework to generate DAG based federated execution plane at runtime to speed up data processing, providing 50-180x speed up by: reduce data movement to single platforms.reduce ETL overhead to perform large-scale analyticsreduce data duplication and storageexecute data processing on the best available technology, including local JVM stream processing Data application agnosticity run data tasks on multiple platforms without re-platforming the code (move jobs from Hadoop to Spark without changinf the code et large)implement a sustainable AI stratgy by using data pools in-situ Enterprise ready federated learning (in development)     ","version":"Next","tagName":"h3"},{"title":"Architecture and Software stack​","type":1,"pageTitle":"What is Wayang?","url":"/docs/introduction/about#architecture-and-software-stack","content":" Apache Wayang's unique architecture, unlike traditional DBMSs, decouples the physical planning and execution layers, empowering developers to express their data processing logic in a platform-agnostic fashion. This separation of concerns allows developers to focus on the algorithmic aspects of their applications without being constrained by the intricacies of specific processing platforms.          At the bottom layers of the software stack, there are the different data storage mediums and the supported data processing platforms. On top of these, Wayang’s core consists of the following main components: the optimizer, the executor, the monitor, and platform-specific drivers. Wayang currently supports two main APIs: the Java one and the Scala one. A Python API is currently under development. Besides using any of the supported languages, users can directly input SQL queries via the SQL library, which transforms them into a Wayang plan. Wayang also comes with an ML library for running ML tasks. Users can directly utilize the provided algorithms or can implement their own algorithm using a simple ML abstraction. To enable support for more programming languages in an efficient way, Wayang will soon come with a Polyglot library.    Apache Wayang's core strength lies in its cross-platform task execution, enabling developers to seamlessly leverage the strengths of various processing engines, such as Hadoop, Spark, and Flink, without sacrificing performance or flexibility. The platform's ease of use further enhances its appeal, making it a compelling choice for data engineers and developers seeking a unified and versatile data processing solution.    Below you can see on the left, a Wayang plan representing the stochastic gradient descent algorithm, which used in most deep learning tasks. On the right, you can see how the optimizer decided to execute it. Orange nodes are the operators that ran on Spark and green the operators executed as a single Java process.        ","version":"Next","tagName":"h3"},{"title":"Query Optimizer​","type":1,"pageTitle":"What is Wayang?","url":"/docs/introduction/about#query-optimizer","content":" Wayang's query optimizer is the principal component of the core. It receives as input a Wayang plan and outputs an execution (platform-specific) plan (see example plan above) with the goal of minimizing the total cost. The metric for the cost can be anything, from execution runtime to monetary cost or energy consumption. To achieve this, the optimizer first “inflates” the plan: For each node that corresponds to a Wayang operator, it adds all the corresponding execution (platform-specific) operators. Once the inflated plan is created, the optimizer attaches not only the operator’s costs but also the costs for moving intermediate data from one platform to another. By default, Wayang uses linear cost formulas to estimate these costs but one can plug their own optimizer, e.g., an ML-based one. At the last step of query optimization, our enumeration algorithm considers available options to output the optimal execution plan w.r.t. a defined cost.        Note that users can control the optimizer by specifying in their code where an operator has to be executed via the withTargetPlatform(plat) call on the desired operator. Then, the optimizer takes into consideration the decisions of the user and outputs an execution plan by navigating a reduced search space during the plan enumeration.    ","version":"Next","tagName":"h3"},{"title":"History​","type":1,"pageTitle":"What is Wayang?","url":"/docs/introduction/about#history","content":" Wayang (formely called Rheem) is the product of many years of top quality research. Below you can find the main publications:  Apache Wayang: A Unified Data Analytics Framework. SIGMOD Rec. 52(3): 30-35 (2023) pdf RHEEMix in the data jungle: a cost-based optimizer for cross-platform systems. VLDB J. 29(6): 1287-1310 (2020) pdf RHEEM: Enabling Cross-Platform Data Processing - May The Big Data Be With You! -. Proc. VLDB Endow. 11(11): 1414-1427 (2018) pdf ","version":"Next","tagName":"h3"},{"title":"Releases","type":0,"sectionRef":"#","url":"/docs/start/download","content":"","keywords":"","version":"Next"},{"title":"0.7.1 (incubating)​","type":1,"pageTitle":"Releases","url":"/docs/start/download#071-incubating","content":" Source: Download Notes: Release Notes  ","version":"Next","tagName":"h2"},{"title":"Frameworks supported​","type":1,"pageTitle":"Releases","url":"/docs/start/download#frameworks-supported","content":" Apache Flink v1.7.1Apache Giraph v1.2.0-hadoop2GraphChi v0.2.2 (only available with scala 11.x)Java Streams (version depends on the java version)JDBC-TemplatePostgres v9.4.1208 (Implementation JDBC-Template)Apache Spark v3.1.2 (scala 12.x) and v2.4.8 (scala 11.x)SQLite3 v3.8.11.2 (implementation JDBC-Template)  NOTE: The supported platforms for Scala may vary depending on the Scala version.\\  ","version":"Next","tagName":"h3"},{"title":"Previous releases​","type":1,"pageTitle":"Releases","url":"/docs/start/download#previous-releases","content":" Older releases can be found here ","version":"Next","tagName":"h3"},{"title":"Getting started","type":0,"sectionRef":"#","url":"/docs/guide/getting-started","content":"","keywords":"","version":"Next"},{"title":"Requirements​","type":1,"pageTitle":"Getting started","url":"/docs/guide/getting-started#requirements","content":" Apache Wayang (incubating) is built upon the foundations of Java 11 and Scala 2.12, providing a robust and versatile platform for data processing applications. If you intend to build Wayang from source, you will also need to have Apache Maven, the popular build automation tool, installed on your system. Additionally, be mindful that some of the processing platforms supported by Wayang may have their own specific installation requirements.  ","version":"Next","tagName":"h2"},{"title":"Get Wayang​","type":1,"pageTitle":"Getting started","url":"/docs/guide/getting-started#get-wayang","content":" Apache Wayang is readily available through Maven Central, facilitating seamless integration into your development workflow. For instance, to utilize Wayang in your Maven-based project, simply add the following dependency to your project's POM file:  &lt;dependency&gt; &lt;groupId&gt;org.apache.wayang&lt;/groupId&gt; &lt;artifactId&gt;wayang-***&lt;/artifactId&gt; &lt;version&gt;0.7.1&lt;/version&gt; &lt;/dependency&gt;   Note the ***: Wayang ships with multiple modules that can be included in your app, depending on how you want to use it:  wayang-core: provides core data structures and the optimizer (required)wayang-basic: provides common operators and data types for your apps (recommended)wayang-api: provides an easy-to-use Scala and Java API to assemble Wayang plans (recommended)wayang-java, wayang-spark, wayang-graphchi, wayang-sqlite3, wayang-postgres: adapters for the various supported processing platformswayang-profiler: provides functionality to learn operator and UDF cost functions from historical execution data  For the sake of version flexibility, you still have to include your Hadoop (hadoop-hdfs and hadoop-common) and Spark (spark-core and spark-graphx) version of choice.  In addition, you can obtain the most recent snapshot version of Wayang via Sonatype's snapshot repository. Just included  &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;sonatype-snapshots&lt;/id&gt; &lt;name&gt;Sonatype Snapshot Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt; &lt;/repository&gt; &lt;repositories&gt;   If you need to rebuild Wayang, e.g., to use a different Scala version, you can simply do so via Maven:  Adapt the version variables (e.g., spark.version) in the main pom.xml file.Build Wayang with the adapted versions. $ mvn clean install Note the standalone profile to fix Hadoop and Spark versions, so that Wayang apps do not explicitly need to declare the corresponding dependencies. Also, note the distro profile, which assembles a binary Wayang distribution. To activate these profiles, you need to specify them when running maven, i.e., mvn clean install -P&lt;profile name&gt;   ","version":"Next","tagName":"h3"},{"title":"Configure Wayang​","type":1,"pageTitle":"Getting started","url":"/docs/guide/getting-started#configure-wayang","content":" To enable Apache Wayang's smooth operation, you need to equip it with details about your processing platforms' capabilities and how to interact with them. A default configuration is available for initial testing, but creating a properties file is generally preferable for fine-tuning the configuration to suit your specific requirements. To harness this personalized configuration effortlessly, launch your application via  $ java -Dwayang.configuration=url://to/my/wayang.properties ...   Essential configuration settings:  General settings wayang.core.log.enabled (= true): whether to log execution statistics to allow learning better cardinality and cost estimators for the optimizerwayang.core.log.executions (= ~/.wayang/executions.json) where to log execution times of operator groupswayang.core.log.cardinalities (= ~/.wayang/cardinalities.json) where to log cardinality measurementswayang.core.optimizer.instrumentation (= org.apache.wayang.core.profiling.OutboundInstrumentationStrategy): where to measure cardinalities in Wayang plans; other options are org.apache.wayang.core.profiling.NoInstrumentationStrategy and org.apache.wayang.core.profiling.FullInstrumentationStrategywayang.core.optimizer.reoptimize (= false): whether to progressively optimize Wayang planswayang.basic.tempdir (= file:///tmp): where to store temporary files, in particular for inter-platform communication Java Streams wayang.java.cpu.mhz (= 2700): clock frequency of processor the JVM runs on in MHzwayang.java.hdfs.ms-per-mb (= 2.7): average throughput from HDFS to JVM in ms/MB Apache Spark spark.master (= local): Spark master various other Spark settings are supported, e.g., spark.executor.memory, spark.serializer, ... wayang.spark.cpu.mhz (= 2700): clock frequency of processor the Spark workers run on in MHzwayang.spark.hdfs.ms-per-mb (= 2.7): average throughput from HDFS to the Spark workers in ms/MBwayang.spark.network.ms-per-mb (= 8.6): average network throughput of the Spark workers in ms/MBwayang.spark.init.ms (= 4500): time it takes Spark to initialize in ms GraphChi wayang.graphchi.cpu.mhz (= 2700): clock frequency of processor GraphChi runs on in MHzwayang.graphchi.cpu.cores (= 2): number of cores GraphChi runs onwayang.graphchi.hdfs.ms-per-mb (= 2.7): average throughput from HDFS to GraphChi in ms/MB SQLite wayang.sqlite3.jdbc.url: JDBC URL to use SQLitewayang.sqlite3.jdbc.user: optional user namewayang.sqlite3.jdbc.password: optional passwordwayang.sqlite3.cpu.mhz (= 2700): clock frequency of processor SQLite runs on in MHzwayang.sqlite3.cpu.cores (= 2): number of cores SQLite runs on PostgreSQL wayang.postgres.jdbc.url: JDBC URL to use PostgreSQLwayang.postgres.jdbc.user: optional user namewayang.postgres.jdbc.password: optional passwordwayang.postgres.cpu.mhz (= 2700): clock frequency of processor PostgreSQL runs on in MHzwayang.postgres.cpu.cores (= 2): number of cores PostgreSQL runs on  To effectively define your applications with Apache Wayang, utilize its Scala or Java API, conveniently found within the wayang-api module. For clear illustrations, refer to the provided examples below.  ","version":"Next","tagName":"h3"},{"title":"Cost Functions​","type":1,"pageTitle":"Getting started","url":"/docs/guide/getting-started#cost-functions","content":" Wayang provides a utility to learn cost functions from historical execution data. Specifically, Wayang can learn configurations for load profile estimators (that estimate CPU load, disk load etc.) for both operators and UDFs, as long as the configuration provides a template for those estimators.  As an example, the JavaMapOperator draws its load profile estimator configuration via the configuration key wayang.java.map.load. Now, it is possible to specify a load profile estimator template in the configuration under the key &lt;original key&gt;.template, e.g.:  wayang.java.map.load.template = {\\ &quot;in&quot;:1, &quot;out&quot;:1,\\ &quot;cpu&quot;:&quot;?*in0&quot;\\ }   This template encapsulates a load profile estimator that requires at minimum one input cardinality and one output cardinality. Furthermore, it simulates CPU load by assuming a direct relationship with the input cardinality. However, more complex functions are possible.  In particular, you can use  the variables in0, in1, ... and out0, out1, ... to incorporate the input and output cardinalities, respectively;operator properties, such as numIterations for the PageRankOperator implementations;the operators +, -, *, /, %, ^, and parantheses;the functions min(x0, x1, ...)), max(x0, x1, ...), abs(x), log(x, base), ln(x), ld(x);and the constants e and pi.  While Apache Wayang provides templates for all execution operators, you will need to explicitly define your user-defined functions (UDFs) by specifying their cost functions, which are based on configuration parameters. This involves creating an initial specification and template for each UDF. As soon as execution data has been collected, you can initiate:  java ... org.apache.wayang.profiler.ga.GeneticOptimizerApp [configuration URL [execution log]]   This tool will attempt to determine suitable values for the question marks (?) within the load profile estimator templates, aligning them with the collected execution data and pre-defined configuration entries for the load profile estimators. These optimized values can then be directly incorporated into your configuration.  ","version":"Next","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Getting started","url":"/docs/guide/getting-started#examples","content":" For some executable examples, have a look at this repository.  ","version":"Next","tagName":"h2"},{"title":"WordCount​","type":1,"pageTitle":"Getting started","url":"/docs/guide/getting-started#wordcount","content":" Java API​  import org.apache.wayang.api.JavaPlanBuilder; import org.apache.wayang.basic.data.Tuple2; import org.apache.wayang.core.api.Configuration; import org.apache.wayang.core.api.WayangContext; import org.apache.wayang.core.optimizer.cardinality.DefaultCardinalityEstimator; import org.apache.wayang.java.Java; import org.apache.wayang.spark.Spark; import java.util.Collection; import java.util.Arrays; public class WordcountJava { public static void main(String[] args){ // Settings String inputUrl = &quot;file:/tmp.txt&quot;; // Get a plan builder. WayangContext wayangContext = new WayangContext(new Configuration()) .withPlugin(Java.basicPlugin()) .withPlugin(Spark.basicPlugin()); JavaPlanBuilder planBuilder = new JavaPlanBuilder(wayangContext) .withJobName(String.format(&quot;WordCount (%s)&quot;, inputUrl)) .withUdfJarOf(WordcountJava.class); // Start building the WayangPlan. Collection&lt;Tuple2&lt;String, Integer&gt;&gt; wordcounts = planBuilder // Read the text file. .readTextFile(inputUrl).withName(&quot;Load file&quot;) // Split each line by non-word characters. .flatMap(line -&gt; Arrays.asList(line.split(&quot;\\\\W+&quot;))) .withSelectivity(10, 100, 0.9) .withName(&quot;Split words&quot;) // Filter empty tokens. .filter(token -&gt; !token.isEmpty()) .withSelectivity(0.99, 0.99, 0.99) .withName(&quot;Filter empty words&quot;) // Attach counter to each word. .map(word -&gt; new Tuple2&lt;&gt;(word.toLowerCase(), 1)).withName(&quot;To lower case, add counter&quot;) // Sum up counters for every word. .reduceByKey( Tuple2::getField0, (t1, t2) -&gt; new Tuple2&lt;&gt;(t1.getField0(), t1.getField1() + t2.getField1()) ) .withCardinalityEstimator(new DefaultCardinalityEstimator(0.9, 1, false, in -&gt; Math.round(0.01 * in[0]))) .withName(&quot;Add counters&quot;) // Execute the plan and collect the results. .collect(); System.out.println(wordcounts); } }   Scala API​  import org.apache.wayang.api._ import org.apache.wayang.core.api.{Configuration, WayangContext} import org.apache.wayang.java.Java import org.apache.wayang.spark.Spark object WordcountScala { def main(args: Array[String]) { // Settings val inputUrl = &quot;file:/tmp.txt&quot; // Get a plan builder. val wayangContext = new WayangContext(new Configuration) .withPlugin(Java.basicPlugin) .withPlugin(Spark.basicPlugin) val planBuilder = new PlanBuilder(wayangContext) .withJobName(s&quot;WordCount ($inputUrl)&quot;) .withUdfJarsOf(this.getClass) val wordcounts = planBuilder // Read the text file. .readTextFile(inputUrl).withName(&quot;Load file&quot;) // Split each line by non-word characters. .flatMap(_.split(&quot;\\\\W+&quot;), selectivity = 10).withName(&quot;Split words&quot;) // Filter empty tokens. .filter(_.nonEmpty, selectivity = 0.99).withName(&quot;Filter empty words&quot;) // Attach counter to each word. .map(word =&gt; (word.toLowerCase, 1)).withName(&quot;To lower case, add counter&quot;) // Sum up counters for every word. .reduceByKey(_._1, (c1, c2) =&gt; (c1._1, c1._2 + c2._2)).withName(&quot;Add counters&quot;) .withCardinalityEstimator((in: Long) =&gt; math.round(in * 0.01)) // Execute the plan and collect the results. .collect() println(wordcounts) } }   ","version":"Next","tagName":"h3"},{"title":"k-means​","type":1,"pageTitle":"Getting started","url":"/docs/guide/getting-started#k-means","content":" Wayang is also capable of iterative processing, which is, e.g., very important for machine learning algorithms, such as k-means.  Scala API​  import org.apache.wayang.api._ import org.apache.wayang.core.api.{Configuration, WayangContext} import org.apache.wayang.core.function.FunctionDescriptor.ExtendedSerializableFunction import org.apache.wayang.core.function.ExecutionContext import org.apache.wayang.core.optimizer.costs.LoadProfileEstimators import org.apache.wayang.java.Java import org.apache.wayang.spark.Spark import scala.util.Random import scala.collection.JavaConversions._ object kmeans { def main(args: Array[String]) { // Settings val inputUrl = &quot;file:/kmeans.txt&quot; val k = 5 val iterations = 100 val configuration = new Configuration // Get a plan builder. val wayangContext = new WayangContext(new Configuration) .withPlugin(Java.basicPlugin) .withPlugin(Spark.basicPlugin) val planBuilder = new PlanBuilder(wayangContext) .withJobName(s&quot;k-means ($inputUrl, k=$k, $iterations iterations)&quot;) .withUdfJarsOf(this.getClass) case class Point(x: Double, y: Double) case class TaggedPoint(x: Double, y: Double, cluster: Int) case class TaggedPointCounter(x: Double, y: Double, cluster: Int, count: Long) { def add_points(that: TaggedPointCounter) = TaggedPointCounter(this.x + that.x, this.y + that.y, this.cluster, this.count + that.count) def average = TaggedPointCounter(x / count, y / count, cluster, 0) } // Read and parse the input file(s). val points = planBuilder .readTextFile(inputUrl).withName(&quot;Read file&quot;) .map { line =&gt; val fields = line.split(&quot;,&quot;) Point(fields(0).toDouble, fields(1).toDouble) }.withName(&quot;Create points&quot;) // Create initial centroids. val random = new Random val initialCentroids = planBuilder .loadCollection(for (i &lt;- 1 to k) yield TaggedPointCounter(random.nextGaussian(), random.nextGaussian(), i, 0)).withName(&quot;Load random centroids&quot;) // Declare UDF to select centroid for each data point. class SelectNearestCentroid extends ExtendedSerializableFunction[Point, TaggedPointCounter] { /** Keeps the broadcasted centroids. */ var centroids: Iterable[TaggedPointCounter] = _ override def open(executionCtx: ExecutionContext) = { centroids = executionCtx.getBroadcast[TaggedPointCounter](&quot;centroids&quot;) } override def apply(point: Point): TaggedPointCounter = { var minDistance = Double.PositiveInfinity var nearestCentroidId = -1 for (centroid &lt;- centroids) { val distance = Math.pow(Math.pow(point.x - centroid.x, 2) + Math.pow(point.y - centroid.y, 2), 0.5) if (distance &lt; minDistance) { minDistance = distance nearestCentroidId = centroid.cluster } } new TaggedPointCounter(point.x, point.y, nearestCentroidId, 1) } } // Do the k-means loop. val finalCentroids = initialCentroids.repeat(iterations, { currentCentroids =&gt; points .mapJava(new SelectNearestCentroid, udfLoad = LoadProfileEstimators.createFromSpecification( &quot;my.udf.costfunction.key&quot;, configuration )) .withBroadcast(currentCentroids, &quot;centroids&quot;).withName(&quot;Find nearest centroid&quot;) .reduceByKey(_.cluster, _.add_points(_)).withName(&quot;Add up points&quot;) .withCardinalityEstimator(k) .map(_.average).withName(&quot;Average points&quot;) }).withName(&quot;Loop&quot;) // Collect the results. .collect() println(finalCentroids) } }  ","version":"Next","tagName":"h3"}],"options":{"id":"default"}}