"use strict";(self.webpackChunkwayang_website=self.webpackChunkwayang_website||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"wayang-release-odysse","metadata":{"permalink":"/blog/wayang-release-odysse","source":"@site/blog/2024-09-01-release-odysse.md","title":"Apache Wayang Release Odysse","description":"Intro","date":"2024-09-01T00:00:00.000Z","formattedDate":"September 1, 2024","tags":[{"label":"wayang","permalink":"/blog/tags/wayang"},{"label":"ASF","permalink":"/blog/tags/asf"},{"label":"release","permalink":"/blog/tags/release"}],"readingTime":15.615,"hasTruncateMarker":false,"authors":[{"name":"Mirko K\xe4mpf","title":"(P)PMC Apache Wayang","url":"https://github.com/kamir","imageURL":"https://avatars.githubusercontent.com/u/1241122?v=4","key":"kamir"}],"frontMatter":{"slug":"wayang-release-odysse","title":"Apache Wayang Release Odysse","authors":"kamir","tags":["wayang","ASF","release"]},"unlisted":false,"nextItem":{"title":"Integrating ML platforms in Wayang","permalink":"/blog/wayang-tensorflow"}},"content":"## Intro\\nThe ASF provides a robust infrastructure for open communities of software developers. We can share ideas, combine forces, contribute code, docs, review-energy, art work, and from time to time we can nail it down. A release defines an intermediate result of the continuous community work.\\n\\nHow we do such a release in the Apache Wayang team is an essential aspect towards graduation. First of all, there are some references to take into account, such as:\\n\\n- https://www.apache.org/legal/release-policy.html\\n- https://maven.apache.org/maven-release/maven-release-plugin/\\n\\nAssuming you are (P)PMC, and assuming that you have the right permissions for such a release, you can follow the path as described in this guide:\\n\\n- [https://plc4x.apache.org/developers/release/release.html](https://plc4x.apache.org/plc4x/latest/developers/release/release.html)\\n\\nI tried to follow exactly this procedure, several times. I failed. Here I share the current status of my __release attempts__.\\n\\nI plan a longer tour, and do not want to block the project for a long time.\\nHence I create this draft, and I hope we can unblock this project as soon as possible.\\n\\n## Status:\\nI am not able to conduct the _mvn release:perform_ step. \\nAnything before worked, sometimes only after some digging, but it worked.\\n\\n* We assume, that due to my membership in two ASF incubator projects I am not able to upload the artefacts to the Nexus repository (H1).\\n\\n* It can be, that I have not the correct user and password in my _settings.xml_ file (H2).\\n  \\nBut I tested a manual login to the nexus server https://repository.apache.org/service/local/staging/deploy/maven2 with success. And beyond that I have no idea how I can verify this detail alone.\\n\\n## Idea / Proposal\\n(1) It would be great, if someone - who has done a release in any other ASF project or in Apache Wayang - could follow the steps I share, so that we can check where the problem hides itself. \\n\\n(2) As a follow-up task, I suggest to add a __Release Guide__ to the Apache Wayang project, including release manager onboarding steps, and checklists for the particular project, derived from the referenced sources which are listed above. \\n\\nBut for now it is all about sharing the status (as I did serveral times on multiple chanels, including JIRA, Slack, Mailing lists) and finding a solution for Apache Wayang release 1.0.\\n\\n## Latest Error:\\n```\\nmvn release:perform -X -DskipTests\\n```\\n```\\n[INFO] Caused by: org.eclipse.aether.deployment.DeploymentException: Failed to deploy artifacts: Could not transfer artifact org.apache.wayang:wayang:pom:1.0.0-RC2 from/to apache.releases.https (https://repository.apache.org/service/local/staging/deploy/maven2): status code: 401, reason phrase: Unauthorized (401)\\n```\\n\\n## Activity Log\\n```bash \\nmvn release:clean\\nmvn versions:set -DnewVersion=1.0.0-RC2\\nmvn versions:commit\\n```\\n\\n```bash \\nmvn release:prepare -Darguments=\'-DskipTests=True\'\\n```\\n\\n```bash \\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-release-plugin:3.0.1:prepare (default-cli) on project wayang: You don\'t have a SNAPSHOT project in the reactor projects list. -> [Help 1]\\n```\\n\\n```bash \\nmvn versions:commit\\nmvn versions:set -DnewVersion=1.0.0-RC2-SNAPSHOT\\nmvn versions:commit\\n```\\n\\n```bash \\nmvn release:prepare -Darguments=\'-DskipTests=True\'\\n```\\n\\n```bash \\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-release-plugin:3.0.1:prepare (default-cli) on project wayang: Cannot prepare the release because you have local modifications :\\n```\\n\\n```bash \\ngit status\\ngit add .\\ngit commit -m \\"prepare for release 1.0.0-RC2-SNAPSHOT\\"\\ngit push\\n```\\n\\n```bash \\nmvn release:prepare -Darguments=\'-DskipTests=True -Dresume=False\' -DdryRun=true\\nmvn release:prepare -Darguments=\'-DskipTests=True -Dresume=False\' -XXX\\n```\\n\\n```bash \\nCaused by: org.eclipse.aether.transfer.NoRepositoryConnectorException: Blocked mirror for repositories: [repository.jboss.org (http://repository.jboss.org/nexus/content/groups/public/, default, releases)]\\n```\\n\\n### Dependency on JDK-11 during release\\n> FIXED with local JDK11 Setup\\n> \\n```\\nbrew install openjdk@11\\n\\ncurl -s \\"https://get.sdkman.io\\" | bash\\nsource \\"$HOME/.sdkman/bin/sdkman-init.sh\\"\\n\\nwhich java\\n\\nwhich java\\n/Users/kamir/.sdkman/candidates/java/current/bin/java\\n\u279c  GITHUB.active export JAVA_HOME=/Users/kamir/.sdkman/candidates/java/current\\n\u279c  GITHUB.active mvn clean -XXX\\nexport JAVA_HOME=\\nsdk install java 11.0.24-amzn\\nsdk home java 11.0.24-amzn\\n\\n/usr/libexec/java_home -v 11\\n\\njenv add /Library/Java/JavaVirtualMachines/jdk-11.0.15.1.jdk/Contents/Home\\njenv global 11.0\\njenv shell 11.0\\njenv local 11.0\\njava -version\\n```\\n\\n### Manual update of release-version\\n\\nDuring the release procedure, do I have to set the version here in this configuration section manually?\\n\\n```xml\\n<plugin>\\n    <groupId>org.apache.maven.plugins</groupId>\\n    <artifactId>maven-release-plugin</artifactId>\\n    <version>3.0.1</version>\\n\\n    <configuration>\\n        <autoVersionSubmodules>true</autoVersionSubmodules>\\n        <autoResolveSnapshots>all</autoResolveSnapshots>\\n        <releaseProfiles>apache-release</releaseProfiles>\\n        \x3c!--<pushChanges>false</pushChanges>--\x3e\\n        \x3c!--<dryRun>true</dryRun>--\x3e\\n        <releaseVersion>0.7.1</releaseVersion>\\n        <updateWorkingCopyVersions>true</updateWorkingCopyVersions>\\n        <updateDependencies>true</updateDependencies>\\n        <tag>wayang-0.7.1</tag>\\n        <scmReleaseCommitComment>@{prefix} prepare release 0.7.1</scmReleaseCommitComment>\\n        <tagNameFormat>apache-@{project.artifactId}-@{project.version}-incubating</tagNameFormat>\\n        <tagNameFormat>v${project.version}</tagNameFormat>\\n    </configuration>\\n</plugin>\\n```\\n\\n> It seems that these properties must be updated manually.\\n\\n### Warning regarding \\"illegal reflective access operation\\"\\n```\\n[ERROR] WARNING: An illegal reflective access operation has occurred\\n[ERROR] WARNING: Illegal reflective access by org.codehaus.groovy.reflection.CachedClass (file:/Users/mkaempf/.m2/repository/org/codehaus/groovy/groovy-all/2.4.9/groovy-all-2.4.9.jar) to method java.lang.Object.finalize()\\n[ERROR] WARNING: Please consider reporting this to the maintainers of org.codehaus.groovy.reflection.CachedClass\\n[ERROR] WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\\n[ERROR] WARNING: All illegal access operations will be denied in a future release\\n```\\n> __This is still an OPEN ISSUE !__\\n\\n### RAT Check fails\\n```\\n[INFO] [ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.13:check (license-check) on project wayang: Too many files with unapproved license: 1 See RAT report in: /Users/mkaempf/GITHUB.private/incubator-wayang/target/rat.txt -> [Help 1]\\n```\\n\\n```cat /Users/mkaempf/GITHUB.private/incubator-wayang/target/rat.txt\\n\\n*****************************************************\\n\\nPrinting headers for text files without a valid license header...\\n \\n=====================================================\\n== File: .java-version\\n=====================================================\\n11.0\\n```\\n> _FIXED by adding .java-versions to .gitignore_\\n\\n### Tag could not be created in SCM.\\n\\n```\\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-release-plugin:3.0.1:prepare (default-cli) on project wayang: Unable to tag SCM\\n[ERROR] Provider message:\\n[ERROR] The git-tag command failed.\\n[ERROR] Command output:\\n[ERROR] fatal: tag \'wayang-0.7.1\' already exists\\n```\\n\\n> FIXED by manual changes in pom.xml.\\n\\n```xml\\n\\n<scm>\\n<connection>scm:git:https://gitbox.apache.org/repos/asf/incubator-wayang.git</connection>\\n<developerConnection>scm:git:https://gitbox.apache.org/repos/asf/incubator-wayang.git</developerConnection>\\n<url>https://github.com/apache/incubator-wayang</url>\\n<tag>1.0.0-RC2-SNAPSHOT</tag>\\n</scm>\\n```\\n\\n```xml\\n<configuration>\\n<autoVersionSubmodules>true</autoVersionSubmodules>\\n<autoResolveSnapshots>all</autoResolveSnapshots>\\n<releaseProfiles>apache-release</releaseProfiles>\\n\x3c!--<pushChanges>false</pushChanges>--\x3e\\n\x3c!--<dryRun>true</dryRun>--\x3e\\n<releaseVersion>1.0.0-RC2-SNAPSHOT</releaseVersion>\\n<updateWorkingCopyVersions>true</updateWorkingCopyVersions>\\n<updateDependencies>true</updateDependencies>\\n<tag>1.0.0-RC2-SNAPSHOT</tag>\\n<scmReleaseCommitComment>@{prefix} prepare release 1.0.0-RC2-SNAPSHOT</scmReleaseCommitComment>\\n<tagNameFormat>apache-@{project.artifactId}-@{project.version}-incubating</tagNameFormat>\\n<tagNameFormat>v${project.version}</tagNameFormat>\\n</configuration>\\n```\\n\\n\\n\\n```\\nmvn release:prepare -Darguments=\'-DskipTests=True -Dresume=True\'\\n```\\n\\n```\\nmvn clean package\\n\\nmvn release:perform -X -DskipTests\\n\\n- you need your password for the keystore to sign the build artefacts.\\n```\\n\\n> So far so good. But now the sun went down.\\n\\n```\\n[INFO] [ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:3.0.0-M1:deploy (default-deploy) on project wayang: ArtifactDeployerException: Failed to deploy artifacts: Could not transfer artifact org.apache.wayang:wayang:pom:1.0.0-RC2 from/to apache.releases.https (https://repository.apache.org/service/local/staging/deploy/maven2): NullPointerException -> [Help 1]\\n[INFO] org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:3.0.0-M1:deploy (default-deploy) on project wayang: ArtifactDeployerException\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:333)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\\n[INFO]     at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\\n[INFO]     at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\\n[INFO]     at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\\n[INFO]     at org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\\n[INFO]     at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\\n[INFO]     at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\\n[INFO]     at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\\n[INFO]     at java.lang.reflect.Method.invoke (Method.java:566)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\\n[INFO] Caused by: org.apache.maven.plugin.MojoExecutionException: ArtifactDeployerException\\n[INFO]     at org.apache.maven.plugins.deploy.DeployMojo.deployProject (DeployMojo.java:201)\\n[INFO]     at org.apache.maven.plugins.deploy.DeployMojo.execute (DeployMojo.java:159)\\n[INFO]     at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:328)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\\n[INFO]     at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\\n[INFO]     at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\\n[INFO]     at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\\n[INFO]     at org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\\n[INFO]     at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\\n[INFO]     at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\\n[INFO]     at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\\n[INFO]     at java.lang.reflect.Method.invoke (Method.java:566)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\\n[INFO] Caused by: org.apache.maven.shared.transfer.artifact.deploy.ArtifactDeployerException: Failed to deploy artifacts: Could not transfer artifact org.apache.wayang:wayang:pom:1.0.0-RC2 from/to apache.releases.https (https://repository.apache.org/service/local/staging/deploy/maven2): NullPointerException\\n[INFO]     at org.apache.maven.shared.transfer.artifact.deploy.internal.Maven31ArtifactDeployer.deploy (Maven31ArtifactDeployer.java:126)\\n[INFO]     at org.apache.maven.shared.transfer.artifact.deploy.internal.DefaultArtifactDeployer.deploy (DefaultArtifactDeployer.java:79)\\n[INFO]     at org.apache.maven.shared.transfer.project.deploy.internal.DefaultProjectDeployer.deploy (DefaultProjectDeployer.java:190)\\n[INFO]     at org.apache.maven.shared.transfer.project.deploy.internal.DefaultProjectDeployer.deploy (DefaultProjectDeployer.java:134)\\n[INFO]     at org.apache.maven.plugins.deploy.DeployMojo.deployProject (DeployMojo.java:193)\\n[INFO]     at org.apache.maven.plugins.deploy.DeployMojo.execute (DeployMojo.java:159)\\n[INFO]     at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:328)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\\n[INFO]     at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\\n[INFO]     at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\\n[INFO]     at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\\n[INFO]     at org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\\n[INFO]     at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\\n[INFO]     at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\\n[INFO]     at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\\n[INFO]     at java.lang.reflect.Method.invoke (Method.java:566)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\\n[INFO] Caused by: org.eclipse.aether.deployment.DeploymentException: Failed to deploy artifacts: Could not transfer artifact org.apache.wayang:wayang:pom:1.0.0-RC2 from/to apache.releases.https (https://repository.apache.org/service/local/staging/deploy/maven2): NullPointerException\\n[INFO]     at org.eclipse.aether.internal.impl.DefaultDeployer.deploy (DefaultDeployer.java:278)\\n[INFO]     at org.eclipse.aether.internal.impl.DefaultDeployer.deploy (DefaultDeployer.java:202)\\n[INFO]     at org.eclipse.aether.internal.impl.DefaultRepositorySystem.deploy (DefaultRepositorySystem.java:393)\\n[INFO]     at org.apache.maven.shared.transfer.artifact.deploy.internal.Maven31ArtifactDeployer.deploy (Maven31ArtifactDeployer.java:122)\\n[INFO]     at org.apache.maven.shared.transfer.artifact.deploy.internal.DefaultArtifactDeployer.deploy (DefaultArtifactDeployer.java:79)\\n[INFO]     at org.apache.maven.shared.transfer.project.deploy.internal.DefaultProjectDeployer.deploy (DefaultProjectDeployer.java:190)\\n[INFO]     at org.apache.maven.shared.transfer.project.deploy.internal.DefaultProjectDeployer.deploy (DefaultProjectDeployer.java:134)\\n[INFO]     at org.apache.maven.plugins.deploy.DeployMojo.deployProject (DeployMojo.java:193)\\n[INFO]     at org.apache.maven.plugins.deploy.DeployMojo.execute (DeployMojo.java:159)\\n[INFO]     at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:328)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\\n[INFO]     at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\\n[INFO]     at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\\n[INFO]     at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\\n[INFO]     at org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\\n[INFO]     at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\\n[INFO]     at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\\n[INFO]     at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\\n[INFO]     at java.lang.reflect.Method.invoke (Method.java:566)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\\n[INFO] Caused by: org.eclipse.aether.transfer.ArtifactTransferException: Could not transfer artifact org.apache.wayang:wayang:pom:1.0.0-RC2 from/to apache.releases.https (https://repository.apache.org/service/local/staging/deploy/maven2): NullPointerException\\n[INFO]     at org.eclipse.aether.connector.basic.ArtifactTransportListener.transferFailed (ArtifactTransportListener.java:44)\\n[INFO]     at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run (BasicRepositoryConnector.java:417)\\n[INFO]     at org.eclipse.aether.connector.basic.BasicRepositoryConnector.put (BasicRepositoryConnector.java:297)\\n[INFO]     at org.eclipse.aether.internal.impl.DefaultDeployer.deploy (DefaultDeployer.java:271)\\n[INFO]     at org.eclipse.aether.internal.impl.DefaultDeployer.deploy (DefaultDeployer.java:202)\\n[INFO]     at org.eclipse.aether.internal.impl.DefaultRepositorySystem.deploy (DefaultRepositorySystem.java:393)\\n[INFO]     at org.apache.maven.shared.transfer.artifact.deploy.internal.Maven31ArtifactDeployer.deploy (Maven31ArtifactDeployer.java:122)\\n[INFO]     at org.apache.maven.shared.transfer.artifact.deploy.internal.DefaultArtifactDeployer.deploy (DefaultArtifactDeployer.java:79)\\n[INFO]     at org.apache.maven.shared.transfer.project.deploy.internal.DefaultProjectDeployer.deploy (DefaultProjectDeployer.java:190)\\n[INFO]     at org.apache.maven.shared.transfer.project.deploy.internal.DefaultProjectDeployer.deploy (DefaultProjectDeployer.java:134)\\n[INFO]     at org.apache.maven.plugins.deploy.DeployMojo.deployProject (DeployMojo.java:193)\\n[INFO]     at org.apache.maven.plugins.deploy.DeployMojo.execute (DeployMojo.java:159)\\n[INFO]     at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:328)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\\n[INFO]     at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\\n[INFO]     at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\\n[INFO]     at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\\n[INFO]     at org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\\n[INFO]     at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\\n[INFO]     at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\\n[INFO]     at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\\n[INFO]     at java.lang.reflect.Method.invoke (Method.java:566)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\\n[INFO] Caused by: java.lang.NullPointerException\\n[INFO]     at java.util.concurrent.ConcurrentHashMap.putVal (ConcurrentHashMap.java:1011)\\n[INFO]     at java.util.concurrent.ConcurrentHashMap.put (ConcurrentHashMap.java:1006)\\n[INFO]     at org.apache.http.impl.client.BasicCredentialsProvider.setCredentials (BasicCredentialsProvider.java:62)\\n[INFO]     at org.eclipse.aether.transport.http.DeferredCredentialsProvider.getCredentials (DeferredCredentialsProvider.java:67)\\n[INFO]     at org.apache.http.client.protocol.RequestAuthCache.doPreemptiveAuth (RequestAuthCache.java:135)\\n[INFO]     at org.apache.http.client.protocol.RequestAuthCache.process (RequestAuthCache.java:110)\\n[INFO]     at org.apache.http.protocol.ImmutableHttpProcessor.process (ImmutableHttpProcessor.java:133)\\n[INFO]     at org.apache.http.impl.execchain.ProtocolExec.execute (ProtocolExec.java:184)\\n[INFO]     at org.apache.http.impl.execchain.RetryExec.execute (RetryExec.java:89)\\n[INFO]     at org.apache.http.impl.execchain.ServiceUnavailableRetryExec.execute (ServiceUnavailableRetryExec.java:85)\\n[INFO]     at org.apache.http.impl.execchain.RedirectExec.execute (RedirectExec.java:110)\\n[INFO]     at org.apache.http.impl.client.InternalHttpClient.doExecute (InternalHttpClient.java:185)\\n[INFO]     at org.apache.http.impl.client.CloseableHttpClient.execute (CloseableHttpClient.java:72)\\n[INFO]     at org.eclipse.aether.transport.http.HttpTransporter.execute (HttpTransporter.java:485)\\n[INFO]     at org.eclipse.aether.transport.http.HttpTransporter.implPut (HttpTransporter.java:469)\\n[INFO]     at org.eclipse.aether.spi.connector.transport.AbstractTransporter.put (AbstractTransporter.java:107)\\n[INFO]     at org.eclipse.aether.connector.basic.BasicRepositoryConnector$PutTaskRunner.runTask (BasicRepositoryConnector.java:564)\\n[INFO]     at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run (BasicRepositoryConnector.java:414)\\n[INFO]     at org.eclipse.aether.connector.basic.BasicRepositoryConnector.put (BasicRepositoryConnector.java:297)\\n[INFO]     at org.eclipse.aether.internal.impl.DefaultDeployer.deploy (DefaultDeployer.java:271)\\n[INFO]     at org.eclipse.aether.internal.impl.DefaultDeployer.deploy (DefaultDeployer.java:202)\\n[INFO]     at org.eclipse.aether.internal.impl.DefaultRepositorySystem.deploy (DefaultRepositorySystem.java:393)\\n[INFO]     at org.apache.maven.shared.transfer.artifact.deploy.internal.Maven31ArtifactDeployer.deploy (Maven31ArtifactDeployer.java:122)\\n[INFO]     at org.apache.maven.shared.transfer.artifact.deploy.internal.DefaultArtifactDeployer.deploy (DefaultArtifactDeployer.java:79)\\n[INFO]     at org.apache.maven.shared.transfer.project.deploy.internal.DefaultProjectDeployer.deploy (DefaultProjectDeployer.java:190)\\n[INFO]     at org.apache.maven.shared.transfer.project.deploy.internal.DefaultProjectDeployer.deploy (DefaultProjectDeployer.java:134)\\n[INFO]     at org.apache.maven.plugins.deploy.DeployMojo.deployProject (DeployMojo.java:193)\\n[INFO]     at org.apache.maven.plugins.deploy.DeployMojo.execute (DeployMojo.java:159)\\n[INFO]     at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:328)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\\n[INFO]     at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\\n[INFO]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\\n[INFO]     at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\\n[INFO]     at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\\n[INFO]     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\\n[INFO]     at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\\n[INFO]     at org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\\n[INFO]     at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\\n[INFO]     at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\\n[INFO]     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\\n[INFO]     at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\\n[INFO]     at java.lang.reflect.Method.invoke (Method.java:566)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\\n[INFO]     at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\\n[INFO] [ERROR]\\n[INFO] [ERROR]\\n[INFO] [ERROR] For more information about the errors and possible solutions, please read the following articles:\\n[INFO] [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\\n[INFO] [DEBUG] Shutting down adapter factory; available factories [file-lock, rwlock-local, semaphore-local, noop]; available name mappers [discriminating, file-gav, file-hgav, file-static, gav, static]\\n[INFO] [DEBUG] Shutting down \'file-lock\' factory\\n[INFO] [DEBUG] Shutting down \'rwlock-local\' factory\\n[INFO] [DEBUG] Shutting down \'semaphore-local\' factory\\n[INFO] [DEBUG] Shutting down \'noop\' factory\\n[INFO] ------------------------------------------------------------------------\\n[INFO] Reactor Summary for Apache Wayang (incubating) 1.0.0-RC3-SNAPSHOT:\\n[INFO]\\n[INFO] Apache Wayang (incubating) ......................... FAILURE [ 23.626 s]\\n[INFO] Wayang Commons ..................................... SKIPPED\\n[INFO] wayang-utils-profile-db ............................ SKIPPED\\n[INFO] Wayang Core ........................................ SKIPPED\\n[INFO] Wayang Basic ....................................... SKIPPED\\n[INFO] Wayang Platform .................................... SKIPPED\\n[INFO] Wayang Platform Java ............................... SKIPPED\\n[INFO] Wayang Platform Spark .............................. SKIPPED\\n[INFO] Wayang Platform JDBC Template ...................... SKIPPED\\n[INFO] Wayang Platform Postgres ........................... SKIPPED\\n[INFO] Wayang Platform SQLite3 ............................ SKIPPED\\n[INFO] Wayang Platform Giraph ............................. SKIPPED\\n[INFO] Wayang Platform Apache Flink ....................... SKIPPED\\n[INFO] Wayang Platform Generic Jdbc ....................... SKIPPED\\n[INFO] Wayang API ......................................... SKIPPED\\n[INFO] Wayang API Scala-Java .............................. SKIPPED\\n[INFO] Wayang Integration Test ............................ SKIPPED\\n[INFO] Wayang API Python .................................. SKIPPED\\n[INFO] wayang-api-sql ..................................... SKIPPED\\n[INFO] Wayang Profiler .................................... SKIPPED\\n[INFO] Wayang Extensions .................................. SKIPPED\\n[INFO] wayang-iejoin ...................................... SKIPPED\\n[INFO] Wayang - Common resources .......................... SKIPPED\\n[INFO] wayang-benchmark ................................... SKIPPED\\n[INFO] Wayang ML4all ...................................... SKIPPED\\n[INFO] Wayang Project Assembly ............................ SKIPPED\\n[INFO] ------------------------------------------------------------------------\\n[INFO] BUILD FAILURE\\n[INFO] ------------------------------------------------------------------------\\n[INFO] Total time:  24.093 s\\n[INFO] Finished at: 2024-06-25T10:53:32+02:00\\n[INFO] ------------------------------------------------------------------------\\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-release-plugin:3.0.1:perform (default-cli) on project wayang: Maven execution failed, exit code: 1 -> [Help 1]\\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-release-plugin:3.0.1:perform (default-cli) on project wayang: Maven execution failed, exit code: 1\\nat org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:333)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\\nat org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\\nat org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\\nat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\\nat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\\nat org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\\nat org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\\nat org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\\nat org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\\nat org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\\nat org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\\nat org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\\nat org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\\nat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\\nat jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\\nat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\\nat java.lang.reflect.Method.invoke (Method.java:566)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\\nCaused by: org.apache.maven.plugin.MojoExecutionException: Maven execution failed, exit code: 1\\nat org.apache.maven.plugins.release.PerformReleaseMojo.execute (PerformReleaseMojo.java:198)\\nat org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:328)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\\nat org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\\nat org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\\nat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\\nat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\\nat org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\\nat org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\\nat org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\\nat org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\\nat org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\\nat org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\\nat org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\\nat org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\\nat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\\nat jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\\nat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\\nat java.lang.reflect.Method.invoke (Method.java:566)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\\nCaused by: org.apache.maven.shared.release.ReleaseExecutionException: Maven execution failed, exit code: 1\\nat org.apache.maven.shared.release.phase.AbstractRunGoalsPhase.execute (AbstractRunGoalsPhase.java:115)\\nat org.apache.maven.shared.release.phase.RunPerformGoalsPhase.runLogic (RunPerformGoalsPhase.java:127)\\nat org.apache.maven.shared.release.phase.RunPerformGoalsPhase.execute (RunPerformGoalsPhase.java:59)\\nat org.apache.maven.shared.release.DefaultReleaseManager.perform (DefaultReleaseManager.java:325)\\nat org.apache.maven.shared.release.DefaultReleaseManager.perform (DefaultReleaseManager.java:268)\\nat org.apache.maven.plugins.release.PerformReleaseMojo.execute (PerformReleaseMojo.java:196)\\nat org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:328)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\\nat org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\\nat org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\\nat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\\nat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\\nat org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\\nat org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\\nat org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\\nat org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\\nat org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\\nat org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\\nat org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\\nat org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\\nat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\\nat jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\\nat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\\nat java.lang.reflect.Method.invoke (Method.java:566)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\\nCaused by: org.apache.maven.shared.release.exec.MavenExecutorException: Maven execution failed, exit code: 1\\nat org.apache.maven.shared.release.exec.InvokerMavenExecutor.executeGoals (InvokerMavenExecutor.java:129)\\nat org.apache.maven.shared.release.exec.AbstractMavenExecutor.executeGoals (AbstractMavenExecutor.java:70)\\nat org.apache.maven.shared.release.phase.AbstractRunGoalsPhase.execute (AbstractRunGoalsPhase.java:105)\\nat org.apache.maven.shared.release.phase.RunPerformGoalsPhase.runLogic (RunPerformGoalsPhase.java:127)\\nat org.apache.maven.shared.release.phase.RunPerformGoalsPhase.execute (RunPerformGoalsPhase.java:59)\\nat org.apache.maven.shared.release.DefaultReleaseManager.perform (DefaultReleaseManager.java:325)\\nat org.apache.maven.shared.release.DefaultReleaseManager.perform (DefaultReleaseManager.java:268)\\nat org.apache.maven.plugins.release.PerformReleaseMojo.execute (PerformReleaseMojo.java:196)\\nat org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:328)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\\nat org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\\nat org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\\nat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\\nat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\\nat org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\\nat org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\\nat org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\\nat org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\\nat org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\\nat org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\\nat org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\\nat org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\\nat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\\nat jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\\nat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\\nat java.lang.reflect.Method.invoke (Method.java:566)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\\nat org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\\n[ERROR]\\n[ERROR]\\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\\n[DEBUG] Shutting down adapter factory; available factories [file-lock, rwlock-local, semaphore-local, noop]; available name mappers [discriminating, file-gav, file-hgav, file-static, gav, static]\\n[DEBUG] Shutting down \'file-lock\' factory\\n[DEBUG] Shutting down \'rwlock-local\' factory\\n[DEBUG] Shutting down \'semaphore-local\' factory\\n[DEBUG] Shutting down \'noop\' factory\\n```\\n\\nThe null pointer exception indicates problems in _settings.xml_. This could be fixed.\\nBut now, the error changes with and I am still not able to stage the build results.\\n\\n```\\nmvn release:perform -X -DskipTests\\n```\\n```\\n[INFO] Caused by: org.eclipse.aether.deployment.DeploymentException: Failed to deploy artifacts: Could not transfer artifact org.apache.wayang:wayang:pom:1.0.0-RC2 from/to apache.releases.https (https://repository.apache.org/service/local/staging/deploy/maven2): status code: 401, reason phrase: Unauthorized (401)\\n```\\n\\n\\n## Checklist for next iteration:\\n\\nThe error you\'re encountering indicates that there is a problem with deploying the artifacts using the `maven-deploy-plugin`. The root cause of the error is a `NullPointerException` during the deployment process, specifically related to the `org.eclipse.aether.transfer.ArtifactTransferException`.\\n\\nHere\'s a step-by-step guide to troubleshoot and resolve this issue:\\n\\n### 1. Check Maven Settings\\n\\nEnsure that your Maven settings (`settings.xml`) are correctly configured for deployment. Verify that the repository settings and credentials are correctly specified.\\n\\n__[DONE]__\\n\\n### 2. Verify Repository URL\\n\\nMake sure the repository URL in your `pom.xml` or `settings.xml` is correct and reachable. The URL should point to the correct staging repository for deployment.\\n\\n_[OPEN]_ - I did not touch it, so I guess it is correct in the _pom.xml_.\\n\\n### 3. Maven Version Compatibility\\n\\nVerify that you are using a compatible version of Maven. Sometimes, upgrading or downgrading Maven can resolve such issues.\\n\\n``` \\nApache Maven 3.9.6 (bc0240f3c744dd6b6ec2920b3cd08dcc295161ae)\\nMaven home: /usr/local/Cellar/maven/3.9.6/libexec\\nJava version: 11.0.15.1, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk-11.0.15.1.jdk/Contents/Home\\nDefault locale: en_DE, platform encoding: US-ASCII\\nOS name: \\"mac os x\\", version: \\"13.2.1\\", arch: \\"x86_64\\", family: \\"mac\\"\\n```\\n__[DONE]__\\n\\n### 4. Check for Network Issues\\n\\nEnsure there are no network issues that might be causing problems in connecting to the repository. Sometimes, network configurations, firewalls, or proxy settings can interfere with the deployment process.\\n\\n__[DONE]__\\n\\n### 5. Update Maven Plugins\\n\\nEnsure that you are using the latest versions of the Maven plugins. Sometimes, bugs in older versions can cause unexpected issues.\\n\\n_[OPEN]_\\n\\n### 6. Configure the `maven-deploy-plugin` in `pom.xml`\\n\\nMake sure the `maven-deploy-plugin` is correctly configured in your `pom.xml`. Here\'s an example configuration:\\n\\n```xml\\n<plugin>\\n    <groupId>org.apache.maven.plugins</groupId>\\n    <artifactId>maven-deploy-plugin</artifactId>\\n    <version>3.0.0-M1</version>\\n    <configuration>\\n        <repositoryId>apache.releases.https</repositoryId>\\n        <url>https://repository.apache.org/service/local/staging/deploy/maven2</url>\\n    </configuration>\\n</plugin>\\n```\\n\\n### 7. Increase Verbose Logging\\n\\nEnable verbose logging to get more details about the error. You can do this by adding the `-X` flag when running the Maven command:\\n\\n```sh\\nmvn clean deploy -X\\n```\\n__[DONE]__\\n\\n### 8. Retry with a Clean Local Repository\\n\\nSometimes, a corrupt local repository can cause issues. Try cleaning your local Maven repository and re-running the deployment:\\n\\n```sh\\nmvn clean install -U\\nmvn deploy\\n```\\n\\n_[OPEN]_ - Does not match to an authentication issue.\\n\\n### 9. Check for Missing Credentials\\n\\nEnsure that the credentials for the repository are correctly set up in your `settings.xml`:\\n\\n```xml\\n<servers>\\n    <server>\\n        <id>apache.releases.https</id>\\n        <username>your-username</username>\\n        <password>your-password</password>\\n    </server>\\n</servers>\\n```\\n\\n__[DONE]__\\n\\n### 10. Review the Full Stack Trace\\n\\nThe full stack trace indicates a `NullPointerException`:\\n\\n```plaintext\\nCaused by: java.lang.NullPointerException\\n    at java.util.concurrent.ConcurrentHashMap.putVal (ConcurrentHashMap.java:1011)\\n    at java.util.concurrent.ConcurrentHashMap.put (ConcurrentHashMap.java:1006)\\n    at org.apache.http.impl.client.BasicCredentialsProvider.setCredentials (BasicCredentialsProvider.java:62)\\n    at org.eclipse.aether.transport.http.DeferredCredentialsProvider.getCredentials (DeferredCredentialsProvider.java:67)\\n    at org.apache.http.client.protocol.RequestAuthCache.doPreemptiveAuth (RequestAuthCache.java:135)\\n    at org.apache.http.client.protocol.RequestAuthCache.process (RequestAuthCache.java:110)\\n    at org.apache.http.protocol.ImmutableHttpProcessor.process (ImmutableHttpProcessor.java:133)\\n    at org.apache.http.impl.execchain.ProtocolExec.execute (ProtocolExec.java:184)\\n    ...\\n```\\n\\nThis suggests that there might be an issue with how credentials are being handled. Double-check that the credentials are being correctly passed and processed.\\n\\n__[DONE]__"},{"id":"wayang-tensorflow","metadata":{"permalink":"/blog/wayang-tensorflow","source":"@site/blog/2024-05-07-wayang-tensorflow.md","title":"Integrating ML platforms in Wayang","description":"We are happy to announce that we have extended Wayang to be able to utilize any ML platform and any ML operators.","date":"2024-05-07T00:00:00.000Z","formattedDate":"May 7, 2024","tags":[{"label":"wayang","permalink":"/blog/tags/wayang"},{"label":"ML","permalink":"/blog/tags/ml"},{"label":"tensorflow","permalink":"/blog/tags/tensorflow"}],"readingTime":2.77,"hasTruncateMarker":false,"authors":[{"name":"Zoi Kaoudi","title":"(P)PMC Apache Wayang","url":"https://github.com/zkaoudi","imageURL":"https://avatars.githubusercontent.com/zkaoudi","key":"zkaoudi"}],"frontMatter":{"slug":"wayang-tensorflow","title":"Integrating ML platforms in Wayang","authors":["zkaoudi"],"tags":["wayang","ML","tensorflow"]},"unlisted":false,"prevItem":{"title":"Apache Wayang Release Odysse","permalink":"/blog/wayang-release-odysse"},"nextItem":{"title":"Wayang and the Federated AI","permalink":"/blog/wayang-federated-ai"}},"content":"We are happy to announce that we have extended Wayang to be able to utilize any ML platform and any ML operators. \\nThanks to the extensible nature of Wayang, the only core changes we had to do were introducing the concept of a ``Model`` and implement a new driver for the newly added platform.\\n\\n## Step 1: Introducing a Model\\n\\nWith respect to the model, we followed Wayang\u2019s abstraction philosophy: We created a ``Model`` interface to be used as input or output by Wayang operators and then extended it for the platform-specific operators. Different model interfaces can be found here:\\n\\nhttps://github.com/apache/incubator-wayang/tree/main/wayang-commons/wayang-basic/src/main/java/org/apache/wayang/basic/model\\n\\nA platform-specific model needs to be instantiated to be used as the output of a training operator and as input for an inference operator. You can see an example of the ``SparkMLModel`` here:\\n\\nhttps://github.com/apache/incubator-wayang/tree/main/wayang-platforms/wayang-spark/src/main/java/org/apache/wayang/spark/model/SparkMLModel.java \\n\\n\\n## Step 2: Introducing Training Operators\\n\\nWe added the desired Wayang (platform-agnostic) training operators which are binary to unary operators, taking as input the X and y values and outputting a Model. You can find an example of a LinearRegressionOperator here:\\n\\nhttps://github.com/apache/incubator-wayang/blob/main/wayang-commons/wayang-basic/src/main/java/org/apache/wayang/basic/operators/LinearRegressionOperator.java\\n\\nPlatform-specific execution operators, such as SparkLinearRegressionOperator, can be easily added as any other execution operator: extending the corresponding Wayang operator and providing the mappings from the Wayang to the execution operator. See, for example, the ``SparkLinearRegressionOperator``:\\n\\nhttps://github.com/apache/incubator-wayang/tree/main/wayang-platforms/wayang-spark/src/main/java/org/apache/wayang/spark/operators/ml/SparkLinearRegressionOperator.java\\n\\n## Step 3: Introducing Prediction Operators\\n Additionally, we created a ``PredictOperator``, a BinaryToUnary Wayang (platform-agnostic) operator which takes as input the data quanta and a model and outputs the data quanta with the predictions output by the model. \\n\\n https://github.com/apache/incubator-wayang/tree/main/wayang-commons/wayang-basic/src/main/java/org/apache/wayang/basic/operators/PredictOperator.java\\n\\n Then, a concrete platform-specific operator extends from the abstract one. See the ``SparkPredictOperator`` for an example:\\n \\n https://github.com/apache/incubator-wayang/tree/main/wayang-platforms/wayang-spark/src/main/java/org/apache/wayang/spark/operators/ml/SparkPredictOperator.java\\n\\n## Deep Learning Models\\n\\nUnlike traditional machine learning models, the definition of deep learning models is more flexible. Users can combine different blocks (e.g., fully connected blocks, convolutional blocks) to build their desired models. The whole model can be represented as a graph on which the vertices represent blocks and the edges represent connections between blocks. In this case, we built a ``DLModel`` class that implements the ``Model`` interface, which contains a user-defined, platform-agnostic graph of the model:\\n\\nhttps://github.com/apache/incubator-wayang/tree/main/wayang-commons/wayang-basic/src/main/java/org/apache/wayang/basic/model/DLModel.java\\n\\nFor training, we implemented the platform-agnostic ``DLModelTrainingOperator`` Wayang operator:\\n\\nhttps://github.com/apache/incubator-wayang/tree/main/wayang-commons/wayang-basic/src/main/java/org/apache/wayang/basic/operators/DLTrainingOperator.java\\n\\n## New ML platform -- Tensorflow Integration\\nWe have added Tensorflow as a new platform by creating a new module (``wayang-tensorflow``) inside the ``wayang-platforms`` parent module and implementing a Tensorflow driver. The TensorflowExecutor driver is responsible for creating and destroying Tensorflow resources, such as a model graph and a model parameter context. When a training task scheduled on Tensorflow, it will be mapped to TensorflowDLModelTrainingOperator. In this process, the ``DLModel`` will be converted to ``TensorflowModel``, which means that the user-defined model graph will be converted to a Tensorflow model graph. Likewise, for inference, the ``PredictOperator`` will be mapped to ``TensorflowPredictOperator``. All the code for the tensorflow platform can be found here:\\n\\nhttps://github.com/apache/incubator-wayang/tree/main/wayang-platforms/wayang-tensorflow/src/main/java/org/apache/wayang/tensorflow\\n\\n### Acknowledgement\\nThe source code for the support of ML operators and the Tensorflow integration has been contributed by Mingxi Liu.\\n\\n### Follow Wayang\\n\\nApache Wayang is in incubation phase and has a potential roadmap of implementations\\ncoming soon (including the federated learning aspect as well as an SQL interface and a novel\\ndata debugging functionality). If you want to hear or join the community, consult the link\\nhttps://wayang.apache.org/community/ , join the mailing lists, contribute with new ideas,\\nwrite documentation, or fix bugs."},{"id":"wayang-federated-ai","metadata":{"permalink":"/blog/wayang-federated-ai","source":"@site/blog/2024-04-17-federated-ai.md","title":"Wayang and the Federated AI","description":"AI systems and applications are widely used nowadays, from assisting grammar spellings to","date":"2024-04-17T00:00:00.000Z","formattedDate":"April 17, 2024","tags":[{"label":"wayang","permalink":"/blog/tags/wayang"},{"label":"federated","permalink":"/blog/tags/federated"},{"label":"ai","permalink":"/blog/tags/ai"}],"readingTime":2.855,"hasTruncateMarker":false,"authors":[{"name":"Gl\xe1ucia Esppenchutz","title":"(P)PMC Apache Wayang","url":"https://github.com/glauesppen","imageURL":"https://avatars.githubusercontent.com/glauesppen","key":"glauesppen"}],"frontMatter":{"slug":"wayang-federated-ai","title":"Wayang and the Federated AI","authors":["glauesppen"],"tags":["wayang","federated","ai"]},"unlisted":false,"prevItem":{"title":"Integrating ML platforms in Wayang","permalink":"/blog/wayang-tensorflow"},"nextItem":{"title":"Pywayang - Apache Wayang\'s Python API","permalink":"/blog/wayang-python-api"}},"content":"AI systems and applications are widely used nowadays, from assisting grammar spellings to\\ndetecting early signs of cancer cells. Building an AI requires a lot of data and training to achieve\\nthe desired results, and federated learning is an approach to make AI training more viable.\\nFederated learning (or collaborative learning) is a technique that trains AI models on data\\ndistributed across multiple serves or devices. It does so without centralizing data on a single\\nplace or storage. It also prevents the possibility of data breaches and protects sensitive\\npersonal data. One of the significant challenges in working with AI is the variety of tools found\\nin the market or the open-source community. Each tool provides results in a different form;\\nintegrating them can be pretty challenging. Let\'s talk about Apache Wayang (incubating) and\\nhow it can help to solve this problem.\\n\\n## Apache Wayang in the Federated AI world\\n\\nApache Wayang (Wayang, for short), a project in an incubation phase at Apache Software\\nFoundation (ASF), integrates big data platforms and tools by removing the complexity of\\nworrying about low-level details. Interestingly, even if it was not designed for, Wayang could\\nalso serve as a scalable platform for federated learning: the Wayang community is starting to\\nwork on integrating federated learning capabilities. In a federated learning approach, Wayang\\nwould allow different local models to be built and exchange its model results across other data\\ncenters to combine them into a single enhanced model.\\n\\n\\n## A real-world example\\n\\nLet\'s consider a real-world scenario. Hospitals and health organizations have increased their\\ninvestments in machine/deep learning initiatives to learn more and predict diagnostics.\\nHowever, due to legal frameworks, sharing patients\' information or diagnostics is impossible,\\nand the solution would be to apply federated learning. To solve this problem, we could use\\nWayang to help to train the models. See the diagram 1 below:\\n\\n<br/>\\n<img width=\\"75%\\" alt=\\"wayang stack\\" src=\\"/img/architecture/federated-ai-architecture-1.png\\" />\\n<br/><br/>\\n\\nAs a first step, the data scientists would send an ML task to Wayang, which will work as an\\nabstraction layer to connect to different data processing platforms, sparing the time to build\\nintegration code for each. Then, the data platforms process and generate the results that will\\nbe sent back to Wayang. Wayang aggregates the results into one \\"global result\\" and sends it\\nback to the requestor as a next step.\\n\\n<br/>\\n<img width=\\"75%\\" alt=\\"wayang stack\\" src=\\"/img/architecture/federated-ai-architecture-2.png\\" />\\n<br/><br/>\\n\\nThe process repeats until the desired results are achieved.\\nAlthough it is very much like a Federated learning pipeline, Wayang removes a considerable\\nlayer of complexity from the developers by integrating with diverse types of data platforms. It\\nalso brings fast development and reduces the need for a deep understanding of data\\ninfrastructure or integrations. Developers can focus on the logic and how to execute tasks\\ninstead of details about data processors.\\n\\n### Follow Wayang\\n\\nApache Wayang is in an incubation phase and has a potential roadmap of implementations\\ncoming soon (including the federated learning aspect as well as an SQL interface and a novel\\ndata debugging functionality). If you want to hear or join the community, consult the link\\nhttps://wayang.apache.org/community/ , join the mailing lists, contribute with new ideas,\\nwrite documentation, or fix bugs.\\n\\n<br/>\\n\\n##### Thank you!\\nI (Gl\xe1ucia) want to thank professor Jorge Quian\xe9 for the guidance to write this blog post.\\nThanks for incentivate me to join the project and for the knowledge shared. I will always remember you."},{"id":"wayang-python-api","metadata":{"permalink":"/blog/wayang-python-api","source":"@site/blog/2024-04-09-python-api.md","title":"Pywayang - Apache Wayang\'s Python API","description":"In the vast landscape of data processing, efficiency and flexibility are","date":"2024-04-09T00:00:00.000Z","formattedDate":"April 9, 2024","tags":[{"label":"wayang","permalink":"/blog/tags/wayang"},{"label":"python","permalink":"/blog/tags/python"}],"readingTime":3.815,"hasTruncateMarker":true,"authors":[{"name":"Juri Petersen","title":"Apache Committer","url":"https://github.com/juripetersen","imageURL":"https://avatars.githubusercontent.com/u/43411515?v=4","key":"juripetersen"}],"frontMatter":{"slug":"wayang-python-api","title":"Pywayang - Apache Wayang\'s Python API","authors":["juripetersen"],"tags":["wayang","python"]},"unlisted":false,"prevItem":{"title":"Wayang and the Federated AI","permalink":"/blog/wayang-federated-ai"},"nextItem":{"title":"Apache Kafka meets Wayang - Part 3","permalink":"/blog/kafka-meets-wayang-3"}},"content":"In the vast landscape of data processing, efficiency and flexibility are\\nimportant. However, navigating through a multitude of tools and\\nlanguages often is a major inconvenience.\\nApache Wayang\'s upcoming Python API will allow you to seamlessly\\norchestrate data processing tasks without ever leaving the comfort\\nof Python, irrespective of the underlying framework written in Java.\\n\\n## Expanding Apache Wayang\'s APIs\\nApache Wayang\'s architecture decouples the process of planning from the\\nresulting execution, allowing users to specify platform agnostic plans\\nthrough the provided APIs.\\n\\n<br/>\\n<img width=\\"75%\\" alt=\\"wayang stack\\" src=\\"/img/architecture/wayang-stack.png\\" />\\n<br/><br/>\\n\\nPython\'s popularity and convenience for data\\nprocessing workloads makes it an obvious candidate for a desired API.\\nPrevious APIs, such as the Scala API `wayang-api-scala-java` benefited\\nfrom the interoperability of Java and Scala that allows to reuse objects\\nfrom other languages to provide new interfaces. Accessing JVM objects in\\nPython is possible through several libraries, but in doing so,\\nfuture APIs in other programming languages would need similar libraries and\\nimplementations in order to exist. As a contrast to that, providing an\\nAPI within Apache Wayang that receives input plans from any source and\\nexecutes them within allows to create plans and submit them in any\\nprogramming language. The following figure shows the architecture of `pywayang`:\\n\\n<br/>\\n<img width=\\"75%\\" alt=\\"pywayang stack\\" src=\\"/img/architecture/pywayang.png\\" />\\n<br/><br/>\\n\\nThe Python API allows users to specify WayangPlans with UDFs in Python.\\n`pywayang` then serializes the UDFs and constructs the WayangPlan in\\nJSON format, preparing it to be sent to Apache Wayang\'s JSON API.\\nWhen receiving a valid JSON plan, the JSON API uses the optimizer to\\nconstruct an execution plan. However, since UDFs are defined in Python\\nand thus need to be executed in Python as well, an operators function needs to be\\nwrapped into a `WrappedPythonFunction`:\\n\\n```scala\\nval mapOperator = new MapPartitionsOperator[Input, Output](\\n  new MapPartitionsDescriptor[Input, Output](\\n    new WrappedPythonFunction[Input, Output](\\n      ByteString.copyFromUtf8(udf)\\n    ),\\n    classOf[Input],\\n    classOf[Output],\\n  )\\n)\\n```\\n\\nThis wrapped functional descriptor allows to handle execution of\\nUDFs in Python through a socket connection with the `pywayang` worker.\\nInput data is sourced from the platform chosen by the optimizer and Apache\\nWayang handles routing the output data to the next operator.\\n\\n<br/>\\n\\nA new API in any programming languages would have\\nto specify two things:\\n- A way to create plans that conform to a JSON format specified in the\\n  Wayang JSON API.\\n- A `worker` that handles encoding and decoding of user defined\\n  functions (UDFs), as they need to\\n  be executed on iterables in their respective language.\\nAfter that, the API can be added as a module in Wayang, so that\\noperators will be wrapped and UDFs can be executed in the desired\\nprogramming language.\\n\\n\x3c!--truncate--\x3e\\n## Defining WayangPlans in Python\\n\\nAs the \\"Hello World!\\" of data processing systems, wordcount will pose as\\nour primary example to display how users can interact with Apache Wayang\\nthrough the python package `pywayang`.\\n\\n```python\\nfrom pywy.dataquanta import WayangContext\\nfrom pywy.platforms.java import JavaPlugin\\nfrom pywy.platforms.spark import SparkPlugin\\n\\ndef wordcount():\\n    ctx = WayangContext() \\\\\\n        .register({JavaPlugin, SparkPlugin}) \\\\\\n        .textfile(\\"file://README.md\\") \\\\\\n        .flatmap(lambda w: w.split()) \\\\\\n        .filter(lambda w: w.strip() != \\"\\") \\\\\\n        .map(lambda w: (w.lower(), 1)) \\\\\\n        .reduce_by_key(lambda t: t[0], lambda t1, t2: (t1[0], int(t1[1]) + int(t2[1]))) \\\\\\n        .store_textfile(\\"file:///wordcount-out-python.txt\\")\\n\\nif __name__ == \\"__main__\\":\\n    wordcount()\\n```\\n\\nThe example displays a mode of operation that resembles the Scala\\n`PlanBuilder` and the `JavaPlanBuilder`. Plans are specified in a\\nfunctional way, chaining operations until a terminal operation results\\nin execution of the plan.\\n\\n## Wayang-API-JSON\\nThe `wayang-api-json` module provides an executable that starts a REST\\nserver. This server accepts a `WayangPlan` in JSON format.\\nStarting the REST API as a background process can be done by executing\\nthe following:\\n\\n```shell\\nmvn clean package -pl :wayang-assembly -Pdistribution\\ncd wayang-assembly/target/\\ntar -xvf apache-wayang-assembly-0.7.1-SNAPSHOT-incubating-dist.tar.gz\\ncd wayang-0.7.1-SNAPSHOT\\n./bin/wayang-submit org.apache.wayang.api.json.Main &\\n```\\n\\n## Wrapping pipelines in MapPartition operators\\nWith this architecture, the execution of an operator comes with an\\nadditional overhead, because the UDFs will have to be executed in\\npython. Python operators receive iterators through a socket and also\\nreturn their result to Wayang through that connection. To minimize the\\noverhead, unary operators that return unary results will be grouped in\\npipelines. One pipeline of operators will be submitted to the Wayang\\nJSON API as a single `MapPartition` operator. This means that the UDFs\\nspecified in this pipeline can be chained and only on call from Wayang\\nto the Python worker will have to be made for a given pipeline.\\n\\n## Coming soon\\nAs the Python API is currently in development and we are applying\\nfinishing touches, this article serves as an outlook for what users can\\nexpect to see soon.\\n\\nAuthor: [juripetersen](https://github.com/juripetersen)"},{"id":"kafka-meets-wayang-3","metadata":{"permalink":"/blog/kafka-meets-wayang-3","source":"@site/blog/2024-03-10-kafka-meets-wayang-3.md","title":"Apache Kafka meets Wayang - Part 3","description":"The third part of this article series is an activity log.","date":"2024-03-10T00:00:00.000Z","formattedDate":"March 10, 2024","tags":[{"label":"wayang","permalink":"/blog/tags/wayang"},{"label":"kafka","permalink":"/blog/tags/kafka"},{"label":"spark","permalink":"/blog/tags/spark"},{"label":"cross organization data collaboration","permalink":"/blog/tags/cross-organization-data-collaboration"}],"readingTime":5.5,"hasTruncateMarker":false,"authors":[{"name":"Mirko K\xe4mpf","title":"(P)PMC Apache Wayang","url":"https://github.com/kamir","imageURL":"https://avatars.githubusercontent.com/u/1241122?v=4","key":"kamir"}],"frontMatter":{"slug":"kafka-meets-wayang-3","title":"Apache Kafka meets Wayang - Part 3","authors":"kamir","tags":["wayang","kafka","spark","cross organization data collaboration"]},"unlisted":false,"prevItem":{"title":"Pywayang - Apache Wayang\'s Python API","permalink":"/blog/wayang-python-api"},"nextItem":{"title":"Apache Wayang vs. Presto/Trino","permalink":"/blog/wayang-vs-trino"}},"content":"The third part of this article series is an activity log. \\nMotivated by the learnings from last time, I stated implementing a Kafka Source component and a Kafka Sink component for the Apache Spark platform in Apache Wayang.\\nIn our previous article we shared the results of the work on the frist Apache Kafka integration using the Java Platform. \\n\\nLet\'s see how it goes this time with Apache Spark.\\n\\n## The goal of this implementation\\n\\nWe want to process data from Apache Kafka topics, which are hosted on Confluent cloud.\\nIn our example scenario, data is available in multiple different clusters, in different regions and owned by different organizations.\\nEach organization uses the [\\"stream sharing\\" feature](https://docs.confluent.io/cloud/current/stream-sharing/index.html) provided by Confluent cloud.\\n\\nThis way, the operator of our central processing job has been granted appropriate permissions. The plaftorm provided the necessary configuration properties, including access coordinates and credentials in the name of the topic owner to us.\\n\\nThe following illustration has already been introduced in part one of the blog series, but for clarity we repeat it here. \\n\\n![images/image-1.png](images/image-1.png)\\n\\nToday, we focus on **Job 4** in the image. We implement a program which uses data federation based on multiple sources. \\nEach source allows us to read the data from that particular topic so that we can process it in a different governance context.\\nIn this example it is a public processing context, in which data from multiple private processing contexts are used together.\\n\\nThis use case is already prepared for high processing loads We can utilize the scalability capabilities of Apache Spark or simply use a Java program for initial tests of the solution. Switching between both is done in one line of code in Apache Wayang. \\n\\nAgain, we start with a **WayangContext**, as shown by examples in the Wayang code repository.\\n\\n```\\nWayangContext wayangContext = new WayangContext().with(Spark.basicPlugin());\\n```\\nWe simply switched the backend system towards Apache Spark by using the _WayangContext_ with _Spark.basicPlugin()_.\\nThe **JavaPlanBuilder** and all other logic of our example job won\'t be touched.\\n\\nIn order to make this working we will now implement the Mappings and the Operators for the Apache Spark platform module.\\n\\n## Implementation of Input- and Output Operators\\n\\nWe reuse the Kafka Source and Kafka Sink components which have been created for the JavaKafkaSource and JavaKafkaSink.\\nHence we work with Wayang\'s Java API.\\n\\n**Level 1 \u2013 Wayang execution plan with abstract operators**\\n\\nSince the _JavaPlanBuilder_ already exposes the function for selecting a Kafka topic as source\\nand the _DataQuantaBuilder_ class exposes the _writeKafkaTopic_ function we can move on quickly. \\n\\nRemember, in this API layer we use the Scala programming language, but we utilize the Java classes, implemented in the layer below.\\n\\n**Level 2 \u2013 Wiring between Platform Abstraction and Implementation**\\n\\nAs in the case with the Java Platform, in the second layer we build a bridge between the WayangContext and the PlanBuilders, which work together with DataQuanta and the DataQuantaBuilder.\\n\\nWe must provide the mapping between the abstract components and the specific implementations in this layer.\\n\\nTherefore, the mappings package in project **wayang-platforms/wayang-spark** has a class _Mappings_ in which \\nour _KafkaTopicSinkMapping_ and _KafkaTopicSourceMapping_ will be registered.\\n\\nAgain, these classes allow the Apache Wayang framework to use the Java implementation of the KafkaTopicSource component (and KafkaTopicSink respectively). \\n\\nWhile the Wayang execution plan uses the higher abstractions, here on the \u201cplatform level\u201d we have to link the specific implementation for the target platform. \\nIn this case this leads to an Apache Spark job, running on a Spark cluster which is set up by the Apache Wayang framework using the logical components of the execution plan, and the Apache Spark configuration provided at runtime.\\n\\nA mapping links an operator implementation to the abstraction used in an execution plan. \\nWe define two new mappings for our purpose, namely KafkaTopicSourceMapping, and KafkaTopicSinkMapping, both could be reused from last round.\\n\\nFor the Spark platform we simply replace the occurences of _JavaPlatform_ with _SparkPlatform_.\\n\\nFurthermore, we create an implementation of the _SparkKafkaTopicSource_ and _SparkKafkaTopicSink_.\\n\\n**Layer 3 \u2013 Input/Output Connector Layer**\\n\\nLet\'s quickly recap, how does Apache Spark interacts with Apache Kafka? \\n\\nThere is already an integration which gives us a DataSet using the Spark SQL framework. \\nFor Spark Streaming, there is also a Kafka integration using the _SparkSession_\'s _readStream()_ function.\\nKafka client properties are provided as key value pairs _k_ and _v_ by using the _option( k, v )_ function.\\nFor writing into a topic, we can use the _writeStream()_ function.\\nBut from a first look, it seems to be not the best fit. \\n\\nAnother approach is possible. \\nWe can use simple RDDs to process data previously consumed from Apache Kafka.\\nThis is a more low-level approach compared to using Datasets with Spark Structured Streaming, \\nand it typically involves using the Kafka RDD API provided by Spark. \\n\\nThis approach is less common with newer versions of Spark, as Structured Streaming provides a higher-level abstraction that simplifies stream processing. \\nHowever, we might need that approach for the integration with Apache Wayang. \\n\\nFor now, we will focus on the lower level approach and plan to consume data from Kafka using a Kafka client, and then\\nwe parallelize the records in an RDD.\\n\\nThis allows us to reuse _KafkaTopicSource_ and _KafkaTopicSink_ classes we built last time. \\nThose were made specifically for a simple non parallel Java program, using one Consumer and one Producer.\\n\\nThe selected approach does not yet fully take advantage from Spark\'s parallelism at load time. \\nFor higher loads and especially for streaming processing we would have to investigate another approache, using a _SparkStreamingContext_, but this is out of scope for now.\\n\\nSince we can\'t reuse the _JavaKafkaTopicSource_ and _JavaKafkaTopicSink_ we rather implement _SparkKafkaTopicSource_ and _SparkKafkaTopicSink_ based on given _SparkTextFileSource_ and _SparkTextFileSink_ which both cary all needed RDD specific logic.\\n\\n## Summary\\nAs expected, the integration of Apache Spark with Apache Wayang was no magic, thanks to a fluent API design and a well structured architecture of Apache Wayang. \\nWe could easily follow the pattern we have worked out in the previous exercise.\\n\\nBut a bunch of much more interesting work will follow next. \\nMore testing, more serialization schemes, and Kafka Schema Registry support should follow, and full parallelization as well.\\n\\nThe code has been submitted to the Apache Wayang repository.\\n\\n\\n## Outlook\\nThe next part of the article series will cover the real world example as described in image 1.\\nWe will show how analysts and developers can use the Apache Kafka integration for Apache Wayang to solve cross organizational collaboration issues.\\nTherefore, we will bring all puzzles together, and show the full implementation of the multi organizational data collaboration use case."},{"id":"wayang-vs-trino","metadata":{"permalink":"/blog/wayang-vs-trino","source":"@site/blog/2024-03-08-wayang-vs-presto.md","title":"Apache Wayang vs. Presto/Trino","description":"We have been asked several times about the difference between Apache Wayang and Presto/Trino. In this blog post, we will clarify the main differences and how they impact various applications and use cases.","date":"2024-03-08T00:00:00.000Z","formattedDate":"March 8, 2024","tags":[{"label":"wayang","permalink":"/blog/tags/wayang"},{"label":"presto","permalink":"/blog/tags/presto"},{"label":"trino","permalink":"/blog/tags/trino"}],"readingTime":2.17,"hasTruncateMarker":true,"authors":[{"name":"Zoi Kaoudi","title":"(P)PMC Apache Wayang","url":"https://github.com/zkaoudi","imageURL":"https://avatars.githubusercontent.com/zkaoudi","key":"zkaoudi"}],"frontMatter":{"slug":"wayang-vs-trino","title":"Apache Wayang vs. Presto/Trino","authors":["zkaoudi"],"tags":["wayang","presto","trino"]},"unlisted":false,"prevItem":{"title":"Apache Kafka meets Wayang - Part 3","permalink":"/blog/kafka-meets-wayang-3"},"nextItem":{"title":"Apache Kafka meets Wayang - Part 2","permalink":"/blog/kafka-meets-wayang-2"}},"content":"We have been asked several times about the difference between Apache Wayang and Presto/Trino. In this blog post, we will clarify the main differences and how they impact various applications and use cases.\\n\\n\x3c!--truncate--\x3e\\n## Key Distinctions\\n\\nTrino/Presto is a **query engine** for **distributed SQL query processing**. It is composed of a coordinator and multiple workers. The coordinator consists of a query optimizer and a scheduler, while the workers are responsible for performing the necessary query processing. Data is fetched from external systems via a Connector API, i.e., Trino/Presto supports [multiple data sources](https://trino.io/ecosystem/data-source). Notably,query processing is is conducted exclusively by Trino/Presto workers, not the external systems.\\n\\n\\nIn contrast, Wayang is a **middleware** for **integrating diverse data platforms**, including but not limited to query engines. This means that Wayang leverages the processing capabilities of the underlying data platforms to complete a given job, with no actual query processing taking place within Wayang itself.\\n\\nBelow you can graphically see the difference between the two systems. Note that not all available data sources or data platforms are illustrated for simplicity reasons.\\n\\nBelow you can see how Wayang integrates data platforms and utilizes them for any data processing required.\\n<br/>\\n<img width=\\"90%\\" alt=\\"Wayang\\" src=\\"/img/blog/wayang-architecture.png\\" title=\\"Wayang\\" />  \\n<br/>\\n<br/>\\n\\nBelow you can see how Trino unifies different data sources and then performs data processing in a distributed manner.\\n<br/>\\n<img width=\\"90%\\" alt=\\"Trino\\" src=\\"/img/blog/trino-architecture.png\\" title=\\"Trino\\"/>  \\n<br/>\\n\\n\\nI hope this makes it clear now. <br/>\\nIn fact, Trino can be easily plugged to Wayang as a platform and be seamlessly integrated with other data platforms, as shown below.\\n\\n<img width=\\"75%\\" alt=\\"Trino\\" src=\\"/img/blog/wayang-with-trino.png\\" />  \\n\\n## What are the advantages of using Wayang?\\n\\nWayang brings several benefits thanks to its integration layer:\\n\\n* Seamless integration of SQL query engines with ML and other data analysis systems within a single job, eliminating the need to materialize intermediate results.\\n\\n\\n* Users are freed from the task of specifying the query engines for an application if they desire. By submitting their Wayang job, the cross-platform optimizer can automatically determine the best data platform to use for improved performance or cost savings.\\n\\n\\n* Wayang facilitates cross-platform data processing by utilizing multiple data platforms to execute a query for a single job, optimizing performance and cost efficiency.\\n\\n* Data does not have to be transferred outside their original location.\\n\\n## Conclusion\\n\\nTrino is a distributed SQL query engine which performs all the query processing of an input SQL query in a distributed manner. Wayang, on the other hand, is a data platform integrator which can automatically determine which data platform(s) is best suited for an application.\\n\\n\\nAuthor: [zkaoudi](https://github.com/zkaoudi)"},{"id":"kafka-meets-wayang-2","metadata":{"permalink":"/blog/kafka-meets-wayang-2","source":"@site/blog/2024-03-06-kafka-meets-wayang-2.md","title":"Apache Kafka meets Wayang - Part 2","description":"In the second part of the article series we describe the implementation of the Kafka Source and Kafka Sink component for Apache Wayang.","date":"2024-03-06T00:00:00.000Z","formattedDate":"March 6, 2024","tags":[{"label":"wayang","permalink":"/blog/tags/wayang"},{"label":"kafka","permalink":"/blog/tags/kafka"},{"label":"cross organization data collaboration","permalink":"/blog/tags/cross-organization-data-collaboration"}],"readingTime":5.095,"hasTruncateMarker":false,"authors":[{"name":"Mirko K\xe4mpf","title":"(P)PMC Apache Wayang","url":"https://github.com/kamir","imageURL":"https://avatars.githubusercontent.com/u/1241122?v=4","key":"kamir"}],"frontMatter":{"slug":"kafka-meets-wayang-2","title":"Apache Kafka meets Wayang - Part 2","authors":"kamir","tags":["wayang","kafka","cross organization data collaboration"]},"unlisted":false,"prevItem":{"title":"Apache Wayang vs. Presto/Trino","permalink":"/blog/wayang-vs-trino"},"nextItem":{"title":"Apache Kafka meets Wayang - Part 1","permalink":"/blog/kafka-meets-wayang-1"}},"content":"In the second part of the article series we describe the implementation of the Kafka Source and Kafka Sink component for Apache Wayang.\\nWe look into the \u201cRead- and Write-Path\u201d for our data items, called _DataQuanta_.\\n\\n## Apache Wayang\u2019s Read & Write Path for Kafka topics\\n\\nTo describe the read and write paths for data in the context of the created Apache Wayang code snippet, the primary classes and interfaces we need to understand are as follows:\\n\\n**WayangContext:** This class is essential for initializing the Wayang processing environment.\\nIt allows you to configure the execution environment and register plugins that define which platforms Wayang can use for data processing tasks, such as _Java.basicPlugin()_ for local Java execution.\\n\\n**JavaPlanBuilder:** This class is used to build and define the data processing pipeline (or plan) in Wayang.\\nIt provides a fluent API to specify the operations to be performed on the data, from reading the input to processing it and writing the output.\\n\\n### Read Path\\nThe read path describes how data is ingested from a source into the Wayang processing pipeline:\\n\\n_Reading from Kafka Topic:_ The method _readKafkaTopic(topicName)_ is used to ingest data from a specified Kafka topic.\\nThis is the starting point of the data processing pipeline, where topicName represents the name of the Kafka topic from which data is read.\\n\\n_Data Tokenization and Preparation:_ Once the data is read from Kafka, it undergoes several transformations such as Splitting, Filtering, and Mapping.\\nWhat follows are the procedures known as Reducing, Grouping, Co-Grouping, and Counting.\\n\\n### Write Path\\n_Writing to Kafka Topic:_ The final step in the pipeline involves writing the processed data back to a Kafka topic using _.writeKafkaTopic(...)_.\\nThis method takes parameters that specify the target Kafka topic, a serialization function to format the data as strings, and additional configuration for load profile estimation, which optimizes the writing process.\\n\\nThis read-write path provides a comprehensive flow of data from ingestion from Kafka, through various processing steps, and finally back to Kafka, showcasing a full cycle of data processing within Apache Wayang\'s abstracted environment and is implemented in our example program shown in *listing 1*.\\n\\n## Implementation of Input- and Output Operators\\nThe next section shows how a new pair of operators can be implemented to extend Apache Wayang\u2019s capabilities on the input and output side.\\nWe created the Kafka Source and Kafka Sink components so that our cross organizational data collaboration scenario can be implemented using data streaming infrastructure.\\n\\n**Level 1 \u2013 Wayang execution plan with abstract operators**\\n\\nThe implementation of our Kafka Source and Kafka Sink components for Apache Wayang requires new methods and classes on three layers.\\nFirst of all in the API package.\\nHere we use the JavaPlanBuilder to expose the function for selecting a Kafka topic as the source to be used by client.\\nThe class _JavaPlanBuilder_ in package _org.apache.wayang.api_ in the project *wayang-api/wayang-api-scala-java* exposes our new functionality to our external client.\\nAn instance of the JavaPlanBuilder is used to define the data processing pipeline.\\nWe use its _readKafkaTopic()_ which specifies the source Kafka topic to read from, and for the write path we use the _writeKafkaTopic()_ method.\\nBoth Methods do only trigger activities in the background.\\n\\nFor the output side, we use the _DataQuantaBuilder_ class, which offers an implementation of the writeKafkaTopic function.\\nThis function is designed to send processed data, referred to as DataQuanta, to a specified Kafka topic.\\nEssentially, it marks the final step in a data processing sequence constructed using the Apache Wayang framework.\\n\\nIn the DataQuanta class we implemented the methods writeKafkaTopic and writeKafkaTopicJava which use the KafkaTopicSink class.\\nIn this API layer we use the Scala programming language, but we utilize the Java classes, implemented in the layer below.\\n\\n**Level 2 \u2013 Wiring between Platform Abstraction and Implementation**\\n\\nThe second layer builds the bridge between the WayangContext and PlanBuilders which work together with DataQuanta and the DataQuantaBuilder.\\n\\nAlso, the mapping between the abstract components and the specific implementations are defined in this layer.\\n\\nTherefore, the mappings package has a class _Mappings_ in which all relevant input and output operators are listed.\\nWe use it to register the KafkaSourceMapping and a KafkaSinkMapping for the particular platform, Java in our case.\\nThese classes allow the Apache Wayang framework to use the Java implementation of the KafkaTopicSource component (and KafkaTopicSink respectively).\\nWhile the Wayang execution plan uses the higher abstractions, here on the \u201cplatform level\u201d we have to link the specific implementation for the target platform.\\nIn our case this leads to a Java program running on a JVM which is set up by the Apache Wayang framework using the logical components of the execution plan.\\n\\nThose mappings link the real implementation of our operators the ones used in an execution plan.\\nThe JavaKafkaTopicSource and the JavaKafkaTopicSink extend the KafkaTopicSource and KafkaTopicSink so that the lower level implementation of those classes become available within Wayang\u2019s Java Platform context.\\n\\nIn this layer, the KafkaConsumer class and the KafkaProducer class are used, but both are configured and instantiated in the next layer underneath.\\nAll this is done in the project *wayang-plarforms/wayang-java*.\\n\\n**Layer 3 \u2013 Input/Output Connector Layer**\\n\\nThe _KafkaTopicSource_ and _KafkaTopicSink_ classes build the third layer of our implementation.\\nBoth are implemented in Java programming language.\\nIn this layer, the real Kafka-Client logic is defined.\\nDetails about consumer and producers, client configuration, and schema handling have to be handled here.\\n\\n## Summary\\nBoth classes in the third layer implement the Kafka client logic which is needed by the Wayang-execution plan when external data flows should be established.\\nThe layer above handles the mapping of the components at startup time.\\nAll this wiring is needed to keep Wayang open and flexible so that multiple external systems can be used in a variety of combinations and using multiple target platforms in combinations.\\n\\n## Outlook\\nThe next part of the article series will cover the creation of an Kafka Source and Sink component for the Apache Spark platform, which allows our use case to scale.\\nFinally, in part four we bring all puzzles together, and show the full implementation of the multi organizational data collaboration use case."},{"id":"kafka-meets-wayang-1","metadata":{"permalink":"/blog/kafka-meets-wayang-1","source":"@site/blog/2024-03-05-kafka-meets-wayang-1.md","title":"Apache Kafka meets Wayang - Part 1","description":"Intro","date":"2024-03-05T00:00:00.000Z","formattedDate":"March 5, 2024","tags":[{"label":"wayang","permalink":"/blog/tags/wayang"},{"label":"kafka","permalink":"/blog/tags/kafka"},{"label":"cross organization data collaboration","permalink":"/blog/tags/cross-organization-data-collaboration"}],"readingTime":3.925,"hasTruncateMarker":false,"authors":[{"name":"Mirko K\xe4mpf","title":"(P)PMC Apache Wayang","url":"https://github.com/kamir","imageURL":"https://avatars.githubusercontent.com/u/1241122?v=4","key":"kamir"}],"frontMatter":{"slug":"kafka-meets-wayang-1","title":"Apache Kafka meets Wayang - Part 1","authors":"kamir","tags":["wayang","kafka","cross organization data collaboration"]},"unlisted":false,"prevItem":{"title":"Apache Kafka meets Wayang - Part 2","permalink":"/blog/kafka-meets-wayang-2"},"nextItem":{"title":"Website updated","permalink":"/blog/website_update"}},"content":"## Intro\\n\\nThis article is the first of a four part series about federated data analysis using Apache Wayang.\\nThe first article starts with an introduction of a typical data colaboration scenario which will emerge in our digital future.\\n\\nIn part two and three we will share a summary of our Apache Kafka client implementation for Apache Wayang.\\nWe started with the Java Platform (part 2) and the Apache Spark implementation follows (W.I.P.) in part three.\\n\\nThe use case behind this work is an imaginary data collaboration scenario.\\nWe see this example and the demand for a solution already in many places.\\nFor us this is motivation enough to propose a solution.\\nThis would also allow us to do more local data processing, and businesses can stop moving data around the world, but rather care about data locality while they expose and share specific information to others by using data federation.\\nThis reduces complexity of data management and cost dramatically.\\n\\nFor this purpose, we illustrate a cross organizational data sharing scenario from the finance sector soon.\\nThis analysis pattern will also be relevant in the context of data analysis along supply chains, another typical example where data from many stakeholder together is needed but never managed in one place, for good reasons.\\n\\nData federation can help us to unlock the hidden value of all those isolated data lakes.\\n\\n\\n## A cross organizational data sharing scenario\\nOur goal is the implementation of a cross organization decentralized data processing scenario, in which protected local data should be processed in combination with public data from public sources in a collaborative manner.\\nInstead of copying all data into a central data lake or a central data platform we decided to use federated analytics.\\nApache Wayang is the tool we work with.\\nIn our case, the public data is hosted on publicly available websites or data pods.\\nA client can use the HTTP(S) protocol to read the data which is given in a well defined format.\\nFor simplicity we decided to use CSV format.\\nWhen we look into the data of each participant we have a different perspective.\\n\\nOur processing procedure should calculate a particular metric on the _local data_ of each participant.\\nAn example of such a metric is the average spending of all users on a particular product category per month.\\nThis can vary from partner to partner, hence, we want to be able to calculate a peer-group comparison so that each partner can see its own metric compared with a global average calculated from contributions by all partners.\\nSuch a process requires global averaging and local averaging.\\nAnd due to governance constraints, we can\u2019t bring all raw data together in one place.\\n\\nInstead, we want to use Apache Wayang for this purpose.\\nWe simplify the procedure and split it into two phases.\\nPhase one is the process, which allows each participant to calculate the local metrics.\\nThis requires only local data. The second phase requires data from all collaborating partners.\\nThe monthly sum and counter values per partner and category are needed in one place by all other parties.\\nHence, the algorithm of the first phase stores the local results locally, and the contributions to the global results in an externally accessible Kafka topic.\\nWe assume this is done by each of the partners.\\n\\nNow we have a scenario, in which an Apache Wayang process must be able to read data from multiple Apache Kafka topics from multiple Apache Kafka clusters but finally writes into a single Kafka topic, which then can be accessed by all the participating clients.\\n\\n![images/image-1.png](images/image-1.png)\\n\\nThe illustration shows the data flows in such a scenario.\\nJobs with red border are executed by the participants in isolation within their own data processing environments.\\nBut they share some of the data, using publicly accessible Kafka topics, marked by A. Job 4 is the Apache Wayang job in our focus: here we intent to read data from 3 different source systems, and write results into a fourth system (marked as B), which can be accesses by all participants again.\\n\\nWith this in mind we want to implement an Apache Wayang application which implements the illustrated *Job 4*.\\nSince as of today, there is now _KafkaSource_ and _KafkaSink_ available in Apache Wayang, an implementation of both will be our first step.\\nOur assumption is, that in the beginning, there won\u2019t be much data.\\n\\nApache Spark is not required to cope with the load, but we expect, that in the future, a single Java application would not be able to handle our workload.\\nHence, we want to utilize the Apache Wayang abstraction over multiple processing platforms, starting with Java.\\nLater, we want to switch to Apache Spark."},{"id":"website_update","metadata":{"permalink":"/blog/website_update","source":"@site/blog/2024-01-25-website_update.md","title":"Website updated","description":"We\'re updated our website and use now Docusaurus.","date":"2024-01-25T00:00:00.000Z","formattedDate":"January 25, 2024","tags":[{"label":"wayang","permalink":"/blog/tags/wayang"}],"readingTime":0.32,"hasTruncateMarker":true,"authors":[{"name":"Alexander Alten","title":"(P)PMC Apache Wayang","url":"https://github.com/2pk03","imageURL":"https://avatars.githubusercontent.com/u/1323575?v=4","key":"alo.alt"}],"frontMatter":{"slug":"website_update","title":"Website updated","authors":["alo.alt"],"tags":["wayang"]},"unlisted":false,"prevItem":{"title":"Apache Kafka meets Wayang - Part 1","permalink":"/blog/kafka-meets-wayang-1"}},"content":"We\'re updated our website and use now Docusaurus.\\n\\n\x3c!--truncate--\x3e\\n# Website updated\\n\\nAuthor: [2pk03](https://github.com/2pk03)\\n\\nWe switched to a new CMS. That\'s all.\\n\\n## Cheatsheet\\n\\nList:\\n- Line one \\n  - Line one.one\\n  - Line one.two\\n- Line two\\n  - Line two.one\\n  - Line two.two\\n- Line three\\n  - ...\\n  - ...\\n\\nAnother style for a list:\\n* Line one\\n* Line two\\n* Line three"}]}')}}]);